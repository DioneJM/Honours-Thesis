{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt') #uncomment if running on new machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Helpful variables\n",
    "EXT_DATA_FOLDER = \"C:\\\\Users\\\\Admin\\\\Projects\\\\thesis\\\\data\\\\\"\n",
    "EXT_DATA_FOLDER2 = \"B:\\\\Datasets\\\\\"\n",
    "\n",
    "ANALYSIS_SAMPLES = os.path.join(EXT_DATA_FOLDER, \"Credibility_Analysis_Samples\\\\September_25\\\\\")\n",
    "dataset_columns = ['Identifier', 'Type', 'Category', 'URL', 'Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5',\n",
    " 'Cat6', 'Cat7', 'Score', 'First date_time', 'Tweets', 'Likes', 'Retweets',\n",
    " 'Potential exposure', 'HTML', 'TEXT']\n",
    "criterias = [\"Cat1\", \"Cat2\", \"Cat3\", \"Cat4\", \"Cat5\", \"Cat6\", \"Cat7\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5', 'Cat6', 'Cat7', 'Category',\n",
       "       'First date_time', 'HTML', 'Identifier', 'Likes', 'Potential Exposure',\n",
       "       'Potential exposure', 'Retweets', 'Score', 'TEXT', 'Tweets', 'Type',\n",
       "       'URL', 'Unnamed: 0', 'Low Credibility'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_articles.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_text_sent = np.array([sent_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_word = np.array([word_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_sent_word = np.array([[word_tokenize(sent) for sent in article] for article in art_text_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Classifier Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for criteria in criterias:\n",
    "    print(\"Training Classifier for \" + criteria + \":\")\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(max_features=10)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               maxj_iter=5, tol=None)),\n",
    "    ])\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    joblib.dump(svm_clf, 'models/svm_clf_tfidf_stop_' + criteria + '.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Credibility Score Detector\n",
    "\n",
    "1) Create a dataset that separates he articles as having low credibility and not low credibility where low credibility is defined to be articles with a credibility score of less than 3\n",
    "\n",
    "2.a) Apply the ensemble of models to classify an article for each category and then use the resulting score to determine whether an article's credibility is low or not\n",
    "\n",
    "2.b) Apply a new model that has been trained to solely just determine whether an article's credibility is low or not\n",
    "\n",
    "3) ???\n",
    "\n",
    "4) Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the manually labelled articles from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470, 21)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "labelled_articles = pd.read_excel(\"dataset5.xlsx\")\n",
    "labelled_articles = labelled_articles.dropna(subset=['TEXT'])\n",
    "labelled_articles = labelled_articles[pd.to_numeric(labelled_articles['Cat1'], errors='coerce').notnull()]\n",
    "for criteria in criterias:\n",
    "    labelled_articles = labelled_articles.dropna(subset=[criteria])\n",
    "print(labelled_articles.shape)\n",
    "\n",
    "#If score < 3 than article isn't credible\n",
    "cred_thresh = 3\n",
    "labelled_articles['Credible'] = np.where(labelled_articles['Score'] < cred_thresh, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Training and testing sets for 2 classification tasks\n",
    "\n",
    "1) Make a training and testing set for classfying whether an article has low credibility or not\n",
    "\n",
    "2) Make a training and testing set for classifying whether an article satisfies each criteria and then using these outputs do determine whether the article has low crediibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = [article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \") for article in labelled_articles[\"TEXT\"]]\n",
    "crit1_labels = labelled_articles[\"Cat1\"]\n",
    "crit2_labels = labelled_articles[\"Cat2\"]\n",
    "crit3_labels = labelled_articles[\"Cat3\"]\n",
    "crit4_labels = labelled_articles[\"Cat4\"]\n",
    "crit5_labels = labelled_articles[\"Cat5\"]\n",
    "crit6_labels = labelled_articles[\"Cat6\"]\n",
    "crit7_labels = labelled_articles[\"Cat7\"]\n",
    "low_cred_labels = list(labelled_articles[\"Credible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "print(len(labelled_articles[labelled_articles['Low Credibility'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Model Approach\n",
    "SVM + TF-IDF with stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'X_train', 'X_test', 'y_train', 'y_test'])\n"
     ]
    }
   ],
   "source": [
    "def svm_build_train(article_text, labels, seed=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(article_text), [int(label) for label in labels], random_state=seed, test_size=0.10)\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    model = {\"model\": svm_clf,\n",
    "             \"X_train\": X_train, \n",
    "             \"X_test\": X_test,\n",
    "             \"y_train\": y_train,\n",
    "             \"y_test\": y_test}\n",
    "\n",
    "    return model\n",
    "\n",
    "print(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Scores vs Actual Scores:\n",
      "[4, 4, 1, 4, 3, 1, 1, 3, 3, 4, 4, 4, 5, 3, 6, 4, 2, 1, 4, 4, 4, 2, 4, 4, 4, 4, 3, 4, 5, 4, 6, 4, 3, 4, 3, 5, 2, 3, 3, 3, 2, 4, 4, 4, 3, 4, 5]\n",
      "[5, 5, 1, 4, 5, 1, 2, 5, 5, 4, 5, 4, 4, 1, 3, 4, 2, 2, 4, 4, 5, 5, 3, 3, 3, 1, 4, 5, 3, 4, 5, 4, 4, 3, 3, 4, 3, 2, 5, 5, 3, 4, 6, 4, 6, 4, 6]\n",
      "% Correct: 0.8723404255319149\n",
      "Average difference when correct: 0.9024390243902439\n",
      "Average difference when incorrect: 1.8333333333333333\n",
      "Micro averaged f1-scores: 0.9361702127659575\n",
      "Micro averaged precision scores: 0.9361702127659575\n",
      "Micro averaged recall scores: 0.9361702127659575\n"
     ]
    }
   ],
   "source": [
    "models = defaultdict(str)\n",
    "seed = random.randint(1,10000)\n",
    "#Train a classifier for all 7 criteria\n",
    "for criteria in criterias:\n",
    "    models[criteria] = svm_build_train(article_text, labelled_articles[criteria], seed)\n",
    "\n",
    "#Replicate the X and y test sets to get the actual credibility score\n",
    "X2_train, X2_test, y2_train, credibility_scores = train_test_split(article_text, list(labelled_articles[\"Score\"]), random_state=seed, test_size=0.10)\n",
    "X2_train, X2_test, y2_train, low_credibility_labels = train_test_split(article_text, list(labelled_articles[\"Credible\"]), random_state=seed, test_size=0.10)\n",
    "\n",
    "#for each criteria, I get the use the model that I trained specifically for it and get it to predict on the articles stored in the X_test set\n",
    "#The results are then stored in 'label_predictions' where it is a 7xN matrix where N is the number of articles in the test sets\n",
    "label_predictions = []\n",
    "for criteria in criterias:\n",
    "    predictions = list(models[criteria]['model'].predict(models[criteria]['X_test']))\n",
    "    label_predictions.append(predictions)\n",
    "\n",
    "#The predictions are then tallied up and compared with the actual credibility score of the article\n",
    "#And the score difference when it correctly and incorrectly identifies\n",
    "predicted_scores = [0 for i in range(len(y_test))]\n",
    "for label in label_predictions:\n",
    "    for article in range(len(label)):\n",
    "        predicted_scores[article] = predicted_scores[article] + label[article]\n",
    "\n",
    "print(\"Predicted Scores vs Actual Scores:\\n{}\\n{}\".format(predicted_scores, credibility_scores))\n",
    "\n",
    "identified = []\n",
    "correct_diff = []\n",
    "incorrect_diff = []\n",
    "for i in range(len(credibility_scores)):\n",
    "    if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh) or (credibility_scores[i] >= cred_thresh and predicted_scores[i] >= cred_thresh)) :\n",
    "        identified.append(1)\n",
    "        correct_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "    else:\n",
    "        incorrect_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "\n",
    "identified_avg = len(identified)/len(credibility_scores)\n",
    "print(\"% Correct: {}\".format(identified_avg))\n",
    "correct_diff_avg = np.array(correct_diff).mean()\n",
    "incorrect_diff_avg = np.array(incorrect_diff).mean()\n",
    "print(\"Average difference when correct: {}\\nAverage difference when incorrect: {}\".format(correct_diff_avg, incorrect_diff_avg))\n",
    "\n",
    "#Calculating the micro averaged f1 score\n",
    "\n",
    "#Converting the predicted scores into a credibility label\n",
    "low_credibility_preds = []\n",
    "for i in range(len(credibility_scores)):\n",
    "        if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh)):\n",
    "            low_credibility_preds.append(0)\n",
    "        else:\n",
    "            low_credibility_preds.append(1)\n",
    "        \n",
    "print(\"Micro averaged f1-scores: {}\".format(f1_score(low_credibility_labels, low_credibility_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged precision scores: {}\".format(precision_score(low_credibility_labels, low_credibility_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged recall scores: {}\".format(recall_score(low_credibility_labels, low_credibility_preds, labels=[0,1], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsvm_predicted = list(svm_clf.predict(X_test))\\nsvm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring=\\'f1_micro\\')\\n\\nprint(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\\njoblib.dump(svm_clf, \\'models/svm_clf_tfidf_stop_\\' + criteria + \\'.joblib\\')\\n'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "svm_predicted = list(svm_clf.predict(X_test))\n",
    "svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "\n",
    "print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "joblib.dump(svm_clf, 'models/svm_clf_tfidf_stop_' + criteria + '.joblib')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple runs of test to get idea of average accuracy and average difference between scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For 500 runs:\n",
      "\n",
      "Average correctly identified %: 0.8780851063829787\n",
      "\n",
      "Average score difference when correctly classified: 0.7126854265117839\n",
      "\n",
      "Average score difference when incorrectly classified: 1.9099660894660895\n",
      "\n",
      "Micro averaged f1-scores: 0.7208936170212767\n",
      "Micro averaged precision scores: 0.7208936170212766\n",
      "Micro averaged recall scores: 0.7208936170212766\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "total_identified = []\n",
    "total_correct_diff = []\n",
    "total_incorrect_diff = []\n",
    "total_low_cred_preds = []\n",
    "total_low_cred_labels = []\n",
    "num_runs = 500\n",
    "for i in range(num_runs):\n",
    "    models = defaultdict(str)\n",
    "    seed = random.randint(1,100000)\n",
    "    #Train a classifier for all 7 criteria\n",
    "    for criteria in criterias:\n",
    "        models[criteria] = svm_build_train(article_text, labelled_articles[criteria], seed)\n",
    "\n",
    "    #Replicate the X and y test sets to get the actual credibility score\n",
    "    X2_train, X2_test, y2_train, credibility_scores = train_test_split(article_text, list(labelled_articles[\"Score\"]), random_state=seed, test_size=0.10)\n",
    "\n",
    "    #for each criteria, I get the use the model that I trained specifically for it and get it to predict on the articles stored in the X_test set\n",
    "    #The results are then stored in 'label_predictions' where it is a 7xN matrix where N is the number of articles in the test sets\n",
    "    label_predictions = []\n",
    "    for criteria in criterias:\n",
    "        predictions = list(models[criteria]['model'].predict(models[criteria]['X_test']))\n",
    "        label_predictions.append(predictions)\n",
    "\n",
    "    #The predictions are then tallied up and compared with the actual credibility score of the article\n",
    "    #And the score difference when it correctly and incorrectly identifies\n",
    "    predicted_scores = [0 for i in range(len(y_test))]\n",
    "    for label in label_predictions:\n",
    "        for article in range(len(label)):\n",
    "            predicted_scores[article] = predicted_scores[article] + label[article]\n",
    "    identified = []\n",
    "    correct_diff = []\n",
    "    incorrect_diff = []\n",
    "    for i in range(len(credibility_scores)):\n",
    "        if((credibility_scores[i] < 3 and predicted_scores[i] < 3) or (credibility_scores[i] >= 3 and predicted_scores[i] >= 3)) :\n",
    "            identified.append(1)\n",
    "            correct_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "        else:\n",
    "            incorrect_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "\n",
    "    identified_avg = len(identified)/len(credibility_scores)\n",
    "    correct_diff_avg = np.array(correct_diff).mean()\n",
    "    incorrect_diff_avg = np.array(incorrect_diff).mean()\n",
    "    \n",
    "    total_identified.append(identified_avg)\n",
    "    total_correct_diff.append(correct_diff_avg)\n",
    "    total_incorrect_diff.append(incorrect_diff_avg)\n",
    "    \n",
    "    #Calculating the micro averaged f1 score\n",
    "\n",
    "    #Converting the predicted scores into a credibility label\n",
    "    low_credibility_preds = []\n",
    "    for i in range(len(credibility_scores)):\n",
    "            if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh)):\n",
    "                low_credibility_preds.append(0)\n",
    "            else:\n",
    "                low_credibility_preds.append(1)\n",
    "    total_low_cred_preds.extend(low_credibility_preds)\n",
    "    total_low_cred_labels.extend(low_credibility_labels)\n",
    "    \n",
    "print(\"\"\"\n",
    "For {} runs:\\n\n",
    "Average correctly identified %: {}\\n\n",
    "Average score difference when correctly classified: {}\\n\n",
    "Average score difference when incorrectly classified: {}\n",
    "\"\"\".format(num_runs, np.array(total_identified).mean(), np.array(total_correct_diff).mean(), np.array(total_incorrect_diff).mean()))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Micro averaged f1-scores: {}\".format(f1_score(total_low_cred_labels, total_low_cred_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged precision scores: {}\".format(precision_score(total_low_cred_labels, total_low_cred_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged recall scores: {}\".format(recall_score(total_low_cred_labels, total_low_cred_preds, labels=[0,1], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
