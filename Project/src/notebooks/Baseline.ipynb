{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Tests\n",
    "\n",
    "This notebook includes the classifiers that will be used to determine the performance of the deep learning-based classifier.\n",
    "\n",
    "### Classifiers:\n",
    "\n",
    "1) Naive Bayes\n",
    "\n",
    "2) Support Vector Machines (SVM)\n",
    "\n",
    "### Performance Measures:\n",
    "\n",
    "1) Storage requirements of the classifier and feature representation used\n",
    "\n",
    "2) Training time of the classifier\n",
    "\n",
    "3) Speed of the classifier\n",
    "\n",
    "4) Accuracy of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt') #uncomment if running on new machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Helpful variables\n",
    "EXT_DATA_FOLDER = \"C:\\\\Users\\\\Admin\\\\Projects\\\\thesis\\\\data\\\\\"\n",
    "EXT_DATA_FOLDER2 = \"B:\\\\Datasets\\\\\"\n",
    "\n",
    "ANALYSIS_SAMPLES = os.path.join(EXT_DATA_FOLDER, \"Credibility_Analysis_Samples\\\\September_25\\\\\")\n",
    "dataset_columns = ['Identifier', 'Type', 'Category', 'URL', 'Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5',\n",
    " 'Cat6', 'Cat7', 'Score', 'First date_time', 'Tweets', 'Likes', 'Retweets',\n",
    " 'Potential exposure', 'HTML', 'TEXT']\n",
    "criterias = [\"Cat1\", \"Cat2\", \"Cat3\", \"Cat4\", \"Cat5\", \"Cat6\", \"Cat7\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in csv and excel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(corpus_path, annotated_samples):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corpus_path: Path for a CSV file containing a list of article URLs and its article text\n",
    "    annotated_samples: Path of the excel file containing articles and its associated URL along with its labels\n",
    "    \n",
    "    Method:\n",
    "    Retrieves the article text by matching the URLs within the corpus_path and annotated_samples and creates a dataframe \n",
    "    containing the URL, article text and the article's corresponding labels.\n",
    "    \n",
    "    Output:\n",
    "    A pandas dataframe\n",
    "    \"\"\"\n",
    "    article_corpus = pd.read_csv(corpus_path)\n",
    "    annotated_corpus = pd.read_excel((annotated_samples))\n",
    "    article_corpus.columns = [\"URL\", \"HTML\", \"TEXT\"]\n",
    "    annotated_articles = annotated_corpus.loc[(annotated_corpus[\"Cat7\"] == 0) | (annotated_corpus[\"Cat7\"] == 1)]\n",
    "    dataset = pd.merge(annotated_articles, article_corpus, how='left', on='URL')\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1116, 3)\n",
      "65    TITLE: Anti-vaccine activists just sparked a U...\n",
      "Name: TEXT, dtype: object\n",
      "Empty DataFrame\n",
      "Columns: [URL, HTML, TEXT]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "article_corpus = pd.read_csv(corpus_path)\n",
    "article_corpus.columns = [\"URL\", \"HTML\", \"TEXT\"]\n",
    "print(article_corpus.shape)\n",
    "\n",
    "print(article_corpus.loc[article_corpus[\"URL\"] == \"https://www.thestar.com/news/world/2017/05/07/anti-vaccine-activists-just-sparked-a-us-states-worst-measles-outbreak-in-decades.html\"][\"TEXT\"])\n",
    "print(article_corpus.loc[article_corpus[\"URL\"] == \"https://www.newscientist.com/article/mg23531335-800-cancer-vaccines-could-prime-our-own-bodies-to-fight-tumours/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&campaign_id=RSS%7CNSNS-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Identifier' 'Type' 'Category' 'URL' 'Cat1' 'Cat2' 'Cat3' 'Cat4' 'Cat5'\n",
      " 'Cat6' 'Cat7' 'Score' 'First date_time' 'Tweets' 'Likes' 'Retweets'\n",
      " 'Potential exposure' 'HTML' 'TEXT']\n",
      "(447, 19)\n"
     ]
    }
   ],
   "source": [
    "corpus_path = os.path.join(EXT_DATA_FOLDER, \"url_text.csv\")\n",
    "excel_files = [\"sample_third_adam_new.xlsx\", \"sample_third_amalie_new.xlsx\", \"sample_third_maryke_new.xlsx\"]\n",
    "\n",
    "df_files = []\n",
    "\n",
    "for filename in excel_files:\n",
    "    annotated_path = os.path.join(ANALYSIS_SAMPLES, filename)\n",
    "    data = create_dataset(corpus_path, annotated_path)\n",
    "    df_files.append(data)\n",
    "    \n",
    "dataset = pd.concat(df_files)\n",
    "\n",
    "print(dataset.columns.values)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September_13\\sample_third_amalie_new.xlsx\n",
      "September_13\\sample_third_maryke_new.xlsx\n"
     ]
    }
   ],
   "source": [
    "for filename in excel_files[1:]:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://triblive.com/news/healthnow/12759008-74/stronger-flu-vaccine-for-elderly-could-help-younger-adults-with-chronic-conditions\n",
      "TITLE: Stronger flu vaccine for elderly could help younger adults with chronic conditions | TribLIVE\n",
      "TEXT:     “Persons who have these conditions have a much greater risk of the flu being more severe to the point of needing to be hospitalized,” Dr. Ken Smith, professor of medicine at Pitt and co-author of the paper, told the Tribune-Review on Thursday. “If you are hospitalized with the flu, your risk of dying is certainly something that is a possibility.”  The high-dose vaccine is recommended for the elderly population because their immune response to the standard-dose vaccine lessens as they age. However, the price for a standard dose is about $11, while the stronger vaccine is about $31 per dose, Smith said. He said the dose for the elderly is about 24 percent stronger than a standard vaccine.   “The growing proportion of middle-aged adults with chronic health conditions coupled with the modest effectiveness of the standard-dose influenza vaccine prompted us to explore whether existing vaccines already recommended for the elderly also could protect younger people,” lead author Jonathan Raviotta, senior research specialist with The Pittsburgh Vaccination Research Group (PittVax) in Pitt's School of Medicine, said in a statement. “Sure enough, expanding the recommendation does seem like a good policy. ... Before making such a recommendation, real world clinical trials are needed.”          \n",
      "0                                                  NaN\n",
      "1    <!DOCTYPE html>\\n<html\\n  xmlns:og=\"http://ogp...\n",
      "2    <!DOCTYPE html>\\n<html lang=\"en-US\" prefix=\"og...\n",
      "3    <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...\n",
      "4    <!DOCTYPE html><html dir=\"ltr\" lang=\"en\" class...\n",
      "Name: HTML, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Example of article with missing text\n",
    "print(dataset.head()[\"URL\"][3])\n",
    "print(dataset.head()[\"TEXT\"][3])  \n",
    "print(dataset.head()[\"HTML\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataset locally\n",
    "writer = pd.ExcelWriter(\"dataset3.xlsx\")\n",
    "dataset.to_excel(writer, \"Sheet1\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470, 21)\n"
     ]
    }
   ],
   "source": [
    "#pre-processing\n",
    "from collections import defaultdict\n",
    "\n",
    "labelled_articles = pd.read_excel(\"dataset5.xlsx\")\n",
    "labelled_articles = labelled_articles.dropna(subset=['TEXT'])\n",
    "labelled_articles = labelled_articles[pd.to_numeric(labelled_articles['Cat1'], errors='coerce').notnull()]\n",
    "\n",
    "for criteria in criterias:\n",
    "    labelled_articles = labelled_articles.dropna(subset=[criteria])\n",
    "\n",
    "print(labelled_articles.shape)\n",
    "art_text_sent = np.array([sent_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_word = np.array([word_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_sent_word = np.array([[word_tokenize(sent) for sent in article] for article in art_text_sent])\n",
    "labels = [labelled_articles[\"Cat1\"], labelled_articles[\"Cat2\"], labelled_articles[\"Cat3\"], labelled_articles[\"Cat4\"], labelled_articles[\"Cat5\"], labelled_articles[\"Cat6\"], labelled_articles[\"Cat7\"]]\n",
    "labels = np.array(labels).transpose()\n",
    "multi_labels = np.array([[int(x) for x in row] for row in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier performance\n",
    "\n",
    "Performance of classifier is measured using an f1_micro score:\n",
    "\n",
    "'f1_micro': Calculate metrics globally by counting the total true positives, false negatives and false positives and accounts for class imbalance. [Source](https://stackoverflow.com/questions/43421456/computing-macro-f1-score-using-sklearn)\n",
    "\n",
    "f1_micro scores are calculated using stratified k-fold cross validation for k = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "categories = ['Not Satisfied', 'Satisfied']\n",
    "nb_bow = []\n",
    "nb_tfidf = []\n",
    "nb_w2v = []\n",
    "svm_bow = []\n",
    "svm_tfidf = []\n",
    "svm_w2v = []\n",
    "\n",
    "nb_optimal = {\n",
    "    \"Cat1\": Pipeline(memory=None,\n",
    "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english')), \n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
    "    \"Cat2\": Pipeline(memory=None,\n",
    "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict', \n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english')), \n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
    "    \"Cat3\": Pipeline(memory=None,\n",
    "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict', \n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english')), \n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
    "    \"Cat4\": Pipeline(memory=None,\n",
    "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english')), \n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
    "    \"Cat5\": Pipeline(memory=None,\n",
    "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
    "    \"Cat6\": Pipeline(memory=None,\n",
    "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english')), \n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
    "    \"Cat7\": Pipeline(memory=None,\n",
    "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "        encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), preprocessor=None, stop_words='english')), \n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
    "}\n",
    "\n",
    "svm_optimal = {\n",
    "    \"Cat1\": None,\n",
    "    \"Cat2\": None,\n",
    "    \"Cat3\": None,\n",
    "    \"Cat4\": None,\n",
    "    \"Cat5\": None,\n",
    "    \"Cat6\": None,\n",
    "    \"Cat7\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB Performance using gridsearch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "NB Average micro f1-score: 0.85 (+/- 0.05)\n",
      "For Cat2:\n",
      "NB Average micro f1-score: 0.77 (+/- 0.05)\n",
      "For Cat3:\n",
      "NB Average micro f1-score: 0.86 (+/- 0.03)\n",
      "For Cat4:\n",
      "NB Average micro f1-score: 0.88 (+/- 0.04)\n",
      "For Cat5:\n",
      "NB Average micro f1-score: 0.65 (+/- 0.09)\n",
      "For Cat6:\n",
      "NB Average micro f1-score: 0.78 (+/- 0.06)\n",
      "For Cat7:\n",
      "NB Average micro f1-score: 0.83 (+/- 0.03)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    svm_clf = svm_optimal[criteria]\\n\\n    svm_clf.fit(X_train, y_train)\\n    svm_predicted = list(svm_clf.predict(X_test))\\n    #print(\"Actual vs. SVM Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\\n\\n    #print(metrics.classification_report(y_test, svm_predicted, target_names=categories))\\n\\n    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring=\\'f1_micro\\')\\n    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\\n    svm_bow.append((svm_cv_scores.mean(), svm_cv_scores.std()))\\n\\nparameters = {\\'vect__ngram_range\\': [(1, 1), (1, 2)],\\n              \\'tfidf__use_idf\\': (True, False),\\n              \\'clf__alpha\\': (1e-2, 1e-3),\\n}\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    nb_clf = nb_optimal[criteria]\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_predicted = list(nb_clf.predict(X_test))\n",
    "    #print(\"Actual vs. NB Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(nb_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, nb_predicted, target_names=categories))\n",
    "\n",
    "    cv_scores = cross_val_score(nb_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    print(\"NB Average micro f1-score: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    nb_bow.append((cv_scores.mean(), cv_scores.std()))\n",
    "\"\"\"\n",
    "    svm_clf = svm_optimal[criteria]\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. SVM Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, svm_predicted, target_names=categories))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    svm_bow.append((svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),}\n",
    "\"\"\"\n",
    "\n",
    "#print(nb_f1_scores)\n",
    "#print(\"====\")\n",
    "#print(svm_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW performance with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "NB Average micro f1-score: 0.84 (+/- 0.04)\n",
      "SVM Average micro f1-score: 0.69 (+/- 0.20)\n",
      "For Cat2:\n",
      "NB Average micro f1-score: 0.76 (+/- 0.03)\n",
      "SVM Average micro f1-score: 0.76 (+/- 0.08)\n",
      "For Cat3:\n",
      "NB Average micro f1-score: 0.85 (+/- 0.02)\n",
      "SVM Average micro f1-score: 0.83 (+/- 0.05)\n",
      "For Cat4:\n",
      "NB Average micro f1-score: 0.88 (+/- 0.05)\n",
      "SVM Average micro f1-score: 0.81 (+/- 0.10)\n",
      "For Cat5:\n",
      "NB Average micro f1-score: 0.63 (+/- 0.09)\n",
      "SVM Average micro f1-score: 0.58 (+/- 0.08)\n",
      "For Cat6:\n",
      "NB Average micro f1-score: 0.78 (+/- 0.08)\n",
      "SVM Average micro f1-score: 0.63 (+/- 0.17)\n",
      "For Cat7:\n",
      "NB Average micro f1-score: 0.82 (+/- 0.04)\n",
      "SVM Average micro f1-score: 0.78 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    nb_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         #('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),])\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_predicted = list(nb_clf.predict(X_test))\n",
    "    #print(\"Actual vs. NB Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(nb_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, nb_predicted, target_names=categories))\n",
    "\n",
    "    cv_scores = cross_val_score(nb_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    print(\"NB Average micro f1-score: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    nb_bow.append((cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         #('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. SVM Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, svm_predicted, target_names=categories))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    svm_bow.append((svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "\n",
    "#print(nb_f1_scores)\n",
    "#print(\"====\")\n",
    "#print(svm_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW Performance without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "NB Average micro f1-score: 0.86 (+/- 0.06)\n",
      "SVM Average micro f1-score: 0.83 (+/- 0.04)\n",
      "For Cat2:\n",
      "NB Average micro f1-score: 0.76 (+/- 0.04)\n",
      "SVM Average micro f1-score: 0.74 (+/- 0.05)\n",
      "For Cat3:\n",
      "NB Average micro f1-score: 0.86 (+/- 0.02)\n",
      "SVM Average micro f1-score: 0.82 (+/- 0.06)\n",
      "For Cat4:\n",
      "NB Average micro f1-score: 0.89 (+/- 0.04)\n",
      "SVM Average micro f1-score: 0.89 (+/- 0.06)\n",
      "For Cat5:\n",
      "NB Average micro f1-score: 0.67 (+/- 0.07)\n",
      "SVM Average micro f1-score: 0.60 (+/- 0.06)\n",
      "For Cat6:\n",
      "NB Average micro f1-score: 0.78 (+/- 0.02)\n",
      "SVM Average micro f1-score: 0.75 (+/- 0.05)\n",
      "For Cat7:\n",
      "NB Average micro f1-score: 0.81 (+/- 0.05)\n",
      "SVM Average micro f1-score: 0.80 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    nb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word')),\n",
    "                         #('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),])\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_predicted = list(nb_clf.predict(X_test))\n",
    "    #print(\"Actual vs. NB Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(nb_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, nb_predicted, target_names=categories))\n",
    "\n",
    "    cv_scores = cross_val_score(nb_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    print(\"NB Average micro f1-score: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    nb_bow.append((cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word')),\n",
    "                         #('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. SVM Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, svm_predicted, target_names=categories))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    svm_bow.append((svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "\n",
    "#print(nb_f1_scores)\n",
    "#print(\"====\")\n",
    "#print(svm_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "NB Average micro f1-score: 0.74 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.82 (+/- 0.05)\n",
      "For Cat2:\n",
      "NB Average micro f1-score: 0.64 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.81 (+/- 0.03)\n",
      "For Cat3:\n",
      "NB Average micro f1-score: 0.87 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.86 (+/- 0.03)\n",
      "For Cat4:\n",
      "NB Average micro f1-score: 0.79 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.89 (+/- 0.03)\n",
      "For Cat5:\n",
      "NB Average micro f1-score: 0.54 (+/- 0.04)\n",
      "SVM Average micro f1-score: 0.61 (+/- 0.06)\n",
      "For Cat6:\n",
      "NB Average micro f1-score: 0.76 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.79 (+/- 0.05)\n",
      "For Cat7:\n",
      "NB Average micro f1-score: 0.84 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.83 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    nb_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),])\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_predicted = list(nb_clf.predict(X_test))\n",
    "    #print(\"Actual vs. NB Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(nb_predicted))\n",
    "\n",
    "\n",
    "    #print(metrics.classification_report(y_test, nb_predicted, target_names=categories))\n",
    "\n",
    "    cv_scores = cross_val_score(nb_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    print(\"NB Average micro f1-score: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    nb_tfidf.append((cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. SVM Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, svm_predicted, target_names=categories))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    svm_tfidf.append((svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "\n",
    "#print(nb_f1_scores)\n",
    "#print(\"====\")\n",
    "#print(svm_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Performance without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "NB Average micro f1-score: 0.74 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.84 (+/- 0.04)\n",
      "For Cat2:\n",
      "NB Average micro f1-score: 0.65 (+/- 0.02)\n",
      "SVM Average micro f1-score: 0.78 (+/- 0.04)\n",
      "For Cat3:\n",
      "NB Average micro f1-score: 0.86 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.86 (+/- 0.03)\n",
      "For Cat4:\n",
      "NB Average micro f1-score: 0.79 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.90 (+/- 0.04)\n",
      "For Cat5:\n",
      "NB Average micro f1-score: 0.59 (+/- 0.04)\n",
      "SVM Average micro f1-score: 0.61 (+/- 0.06)\n",
      "For Cat6:\n",
      "NB Average micro f1-score: 0.76 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.77 (+/- 0.03)\n",
      "For Cat7:\n",
      "NB Average micro f1-score: 0.84 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.82 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    nb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),])\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_predicted = list(nb_clf.predict(X_test))\n",
    "    #print(\"Actual vs. NB Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(nb_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, nb_predicted, target_names=categories))\n",
    "\n",
    "    cv_scores = cross_val_score(nb_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    print(\"NB Average micro f1-score: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    nb_bow.append((cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. SVM Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    #print(metrics.classification_report(y_test, svm_predicted, target_names=categories))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    svm_bow.append((svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "\n",
    "#print(nb_f1_scores)\n",
    "#print(\"====\")\n",
    "#print(svm_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC performance using BoW/TFIDF with and without stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC + BoW with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "LinearSVC Average micro f1-score: 0.80 (+/- 0.09)\n",
      "For Cat2:\n",
      "LinearSVC Average micro f1-score: 0.76 (+/- 0.06)\n",
      "For Cat3:\n",
      "LinearSVC Average micro f1-score: 0.88 (+/- 0.03)\n",
      "For Cat4:\n",
      "LinearSVC Average micro f1-score: 0.88 (+/- 0.07)\n",
      "For Cat5:\n",
      "LinearSVC Average micro f1-score: 0.76 (+/- 0.10)\n",
      "For Cat6:\n",
      "LinearSVC Average micro f1-score: 0.76 (+/- 0.10)\n",
      "For Cat7:\n",
      "LinearSVC Average micro f1-score: 0.82 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "for criteria in criterias:\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         #('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "     verbose=0)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. LinearSVC Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"LinearSVC Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC + TF-IDF with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "LinearSVC Average micro f1-score: 0.84 (+/- 0.06)\n",
      "For Cat2:\n",
      "LinearSVC Average micro f1-score: 0.84 (+/- 0.06)\n",
      "For Cat3:\n",
      "LinearSVC Average micro f1-score: 0.89 (+/- 0.02)\n",
      "For Cat4:\n",
      "LinearSVC Average micro f1-score: 0.88 (+/- 0.04)\n",
      "For Cat5:\n",
      "LinearSVC Average micro f1-score: 0.78 (+/- 0.07)\n",
      "For Cat6:\n",
      "LinearSVC Average micro f1-score: 0.76 (+/- 0.07)\n",
      "For Cat7:\n",
      "LinearSVC Average micro f1-score: 0.82 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "     verbose=0)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. LinearSVC Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"LinearSVC Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC + BoW without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "LinearSVC Average micro f1-score: 0.83 (+/- 0.08)\n",
      "For Cat2:\n",
      "LinearSVC Average micro f1-score: 0.83 (+/- 0.09)\n",
      "For Cat3:\n",
      "LinearSVC Average micro f1-score: 0.90 (+/- 0.04)\n",
      "For Cat4:\n",
      "LinearSVC Average micro f1-score: 0.87 (+/- 0.06)\n",
      "For Cat5:\n",
      "LinearSVC Average micro f1-score: 0.80 (+/- 0.09)\n",
      "For Cat6:\n",
      "LinearSVC Average micro f1-score: 0.80 (+/- 0.11)\n",
      "For Cat7:\n",
      "LinearSVC Average micro f1-score: 0.79 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word')),\n",
    "                         #('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "     verbose=0)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. LinearSVC Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"LinearSVC Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC + TF-IDF without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "LinearSVC Average micro f1-score: 0.80 (+/- 0.07)\n",
      "For Cat2:\n",
      "LinearSVC Average micro f1-score: 0.82 (+/- 0.03)\n",
      "For Cat3:\n",
      "LinearSVC Average micro f1-score: 0.90 (+/- 0.02)\n",
      "For Cat4:\n",
      "LinearSVC Average micro f1-score: 0.86 (+/- 0.04)\n",
      "For Cat5:\n",
      "LinearSVC Average micro f1-score: 0.76 (+/- 0.02)\n",
      "For Cat6:\n",
      "LinearSVC Average micro f1-score: 0.75 (+/- 0.09)\n",
      "For Cat7:\n",
      "LinearSVC Average micro f1-score: 0.81 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "     verbose=0)),\n",
    "    ])\n",
    "\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    #print(\"Actual vs. LinearSVC Predicted labels:\\n\" + str(y_test) + \"\\n\" + str(svm_predicted))\n",
    "\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "    print(\"LinearSVC Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.items())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.items())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(EXT_DATA_FOLDER2, \"glove.6B.300d.txt\"), \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'the'    <map object at 0x00000127D8974390>\n",
       "b','      <map object at 0x00000127D8974C18>\n",
       "b'.'      <map object at 0x00000127D8974B00>\n",
       "b'of'     <map object at 0x00000127D8974A20>\n",
       "b'to'     <map object at 0x00000127D8974EB8>\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_df = pd.DataFrame.from_dict(w2v, orient='index')\n",
    "w2v_df.head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with Mean Embedding Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "NB Average micro f1-score: 0.79 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.60 (+/- 0.28)\n",
      "For Cat2:\n",
      "NB Average micro f1-score: 0.79 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.60 (+/- 0.27)\n",
      "For Cat3:\n",
      "NB Average micro f1-score: 0.92 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.92 (+/- 0.00)\n",
      "For Cat4:\n",
      "NB Average micro f1-score: 0.81 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.60 (+/- 0.29)\n",
      "For Cat5:\n",
      "NB Average micro f1-score: 0.76 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.76 (+/- 0.00)\n",
      "For Cat6:\n",
      "NB Average micro f1-score: 0.66 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.56 (+/- 0.15)\n",
      "For Cat7:\n",
      "NB Average micro f1-score: 0.81 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.60 (+/- 0.29)\n"
     ]
    }
   ],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "\n",
    "for criteria in criterias:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    \n",
    "    \n",
    "    nb_clf = Pipeline([('w2v', MeanEmbeddingVectorizer(w2v)),\n",
    "                         ('clf', MultinomialNB()),])\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "\n",
    "    nb_predicted = nb_clf.predict(X_test)\n",
    "    #print(metrics.classification_report(y_test, nb_predicted, target_names=categories))\n",
    "    nb_cv_scores = cross_val_score(nb_clf, X_train, y_train, scoring='f1_micro')\n",
    "    \n",
    "    \n",
    "    print(\"NB Average micro f1-score: %0.2f (+/- %0.2f)\" % (nb_cv_scores.mean(), nb_cv_scores.std()))\n",
    "    nb_w2v.append((nb_cv_scores.mean(), nb_cv_scores.std()))\n",
    "    \n",
    "    \n",
    "    svm_clf = Pipeline([('w2v', MeanEmbeddingVectorizer(w2v)),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "    \n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = svm_clf.predict(X_test)\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, scoring='f1_micro')\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    svm_w2v.append((svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with Tf-idf Weighted Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cat1:\n",
      "NB Average micro f1-score: 0.80 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.61 (+/- 0.28)\n",
      "For Cat2:\n",
      "NB Average micro f1-score: 0.78 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.60 (+/- 0.26)\n",
      "For Cat3:\n",
      "NB Average micro f1-score: 0.89 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.89 (+/- 0.00)\n",
      "For Cat4:\n",
      "NB Average micro f1-score: 0.81 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.60 (+/- 0.29)\n",
      "For Cat5:\n",
      "NB Average micro f1-score: 0.79 (+/- 0.00)\n",
      "SVM Average micro f1-score: 0.79 (+/- 0.00)\n",
      "For Cat6:\n",
      "NB Average micro f1-score: 0.69 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.56 (+/- 0.18)\n",
      "For Cat7:\n",
      "NB Average micro f1-score: 0.83 (+/- 0.01)\n",
      "SVM Average micro f1-score: 0.83 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "\n",
    "for criteria in criterias:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    print(\"For \" + criteria + \":\")\n",
    "    \n",
    "    \n",
    "    nb_clf = Pipeline([('w2v', TfidfEmbeddingVectorizer(w2v)),\n",
    "                         ('clf', MultinomialNB()),])\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "\n",
    "    nb_predicted = nb_clf.predict(X_test)\n",
    "    #print(metrics.classification_report(y_test, nb_predicted, target_names=categories))\n",
    "    nb_cv_scores = cross_val_score(nb_clf, X_train, y_train, scoring='f1_micro')\n",
    "    \n",
    "    \n",
    "    print(\"NB Average micro f1-score: %0.2f (+/- %0.2f)\" % (nb_cv_scores.mean(), nb_cv_scores.std()))\n",
    "    nb_w2v.append((nb_cv_scores.mean(), nb_cv_scores.std()))\n",
    "\n",
    "    \n",
    "    svm_clf = Pipeline([('w2v', TfidfEmbeddingVectorizer(w2v)),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "    \n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = svm_clf.predict(X_test)\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, scoring='f1_micro')\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    svm_w2v.append((svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of twent_test.data:  <class 'numpy.ndarray'>\n",
      "66\n",
      "[2 2 2 ... 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"type of twent_test.data: \", type(twenty_train.target))\n",
    "print(len(art_text))\n",
    "print(twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pre-trained word2vec\n",
    "pre_word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(datapath(os.path.join(EXT_DATA_FOLDER, \"GoogleNews-vectors-negative300.bin\")), binary=True)\n",
    "pre_word2vec_model.save(\"pre_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'woman' and 'man':  0.76640123\n",
      "Similarity of 'woman' and 'woman':  1.0\n",
      "Similarity of 'dog' and 'hotdog':  0.38931656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity of 'woman' and 'man': \", pre_word2vec_model.similarity('woman', 'man'))\n",
    "print(\"Similarity of 'woman' and 'woman': \", pre_word2vec_model.similarity('woman', 'woman'))\n",
    "print(\"Similarity of 'dog' and 'hotdog': \", pre_word2vec_model.similarity('dog', 'hotdog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gridsearch to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB Parameters\n",
    "\n",
    "nb_parameters = {  \n",
    "    'vec__max_df': (0.5, 0.625, 0.75, 0.875, 1.0),  \n",
    "    'vec__max_features': (None, 5000, 10000, 20000),  \n",
    "    'vec__min_df': (1, 5, 10, 20, 50),  \n",
    "    'tfidf__use_idf': (True, False),  \n",
    "    'tfidf__sublinear_tf': (True, False),  \n",
    "    'vec__binary': (True, False),  \n",
    "    'tfidf__norm': ('l1', 'l2'),  \n",
    "    'clf__alpha': [10 ** x for x in range(-5, 1)]  \n",
    "    }\n",
    "\n",
    "#SVM Parameters \n",
    "\n",
    "svm_parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "\n",
    "best_model = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently finding best model for: Cat1\n",
      "Fitting 10 folds for each of 9600 candidates, totalling 96000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10352 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 11689 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 13106 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14605 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 17845 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 19586 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 21409 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done 23312 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=-1)]: Done 25297 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-1)]: Done 27362 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 29509 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=-1)]: Done 31736 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=-1)]: Done 34045 tasks      | elapsed: 34.9min\n",
      "[Parallel(n_jobs=-1)]: Done 36434 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=-1)]: Done 38905 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=-1)]: Done 41456 tasks      | elapsed: 42.5min\n",
      "[Parallel(n_jobs=-1)]: Done 44089 tasks      | elapsed: 45.2min\n",
      "[Parallel(n_jobs=-1)]: Done 46802 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=-1)]: Done 49597 tasks      | elapsed: 50.8min\n",
      "[Parallel(n_jobs=-1)]: Done 52472 tasks      | elapsed: 53.7min\n",
      "[Parallel(n_jobs=-1)]: Done 55429 tasks      | elapsed: 56.8min\n",
      "[Parallel(n_jobs=-1)]: Done 58466 tasks      | elapsed: 59.9min\n",
      "[Parallel(n_jobs=-1)]: Done 61585 tasks      | elapsed: 63.0min\n",
      "[Parallel(n_jobs=-1)]: Done 64784 tasks      | elapsed: 66.3min\n",
      "[Parallel(n_jobs=-1)]: Done 68065 tasks      | elapsed: 69.7min\n",
      "[Parallel(n_jobs=-1)]: Done 71426 tasks      | elapsed: 73.4min\n",
      "[Parallel(n_jobs=-1)]: Done 74869 tasks      | elapsed: 77.2min\n",
      "[Parallel(n_jobs=-1)]: Done 78392 tasks      | elapsed: 81.2min\n",
      "[Parallel(n_jobs=-1)]: Done 81997 tasks      | elapsed: 85.2min\n",
      "[Parallel(n_jobs=-1)]: Done 85682 tasks      | elapsed: 89.4min\n",
      "[Parallel(n_jobs=-1)]: Done 89449 tasks      | elapsed: 93.6min\n",
      "[Parallel(n_jobs=-1)]: Done 93296 tasks      | elapsed: 98.0min\n"
     ]
    }
   ],
   "source": [
    "for criteria in criterias:\n",
    "    print(\"Currently finding best model for: \" + criteria)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    nb_pipeline = Pipeline([('vec', CountVectorizer(stop_words='english', analyzer='word')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),])\n",
    "    nb_grid_search = GridSearchCV(estimator=nb_pipeline, param_grid=nb_parameters, cv=10, n_jobs=-1, verbose=2)  \n",
    "    nb_grid_search.fit(X_train, y_train)\n",
    "    best_model[criteria] = nb_grid_search\n",
    "    #nb_predicted = list(nb_grid_search.predict(X_test))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
