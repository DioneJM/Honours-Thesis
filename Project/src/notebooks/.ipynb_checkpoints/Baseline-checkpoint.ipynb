{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Tests\n",
    "\n",
    "This notebook includes the classifiers that will be used to determine the performance of the deep learning-based classifier.\n",
    "\n",
    "### Classifiers:\n",
    "\n",
    "1) Naive Bayes\n",
    "\n",
    "2) Support Vector Machines (SVM)\n",
    "\n",
    "### Performance Measures:\n",
    "\n",
    "1) Storage requirements of the classifier and feature representation used\n",
    "\n",
    "2) Training time of the classifier\n",
    "\n",
    "3) Speed of the classifier\n",
    "\n",
    "4) Accuracy of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "\n",
    "# Helpful variables\n",
    "EXT_DATA_FOLDER = \"C:\\\\Users\\\\Admin\\\\Projects\\\\thesis\\\\data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gensim for word2vec\n",
    "\n",
    "#### Inputs\n",
    "Requires a sequence of sentences where the sentence is a list of words:\n",
    "E.g. \"Hi there. Goodbye there\" -> [[\"Hi\", \"there\"], [\"Goodbye\", \"there\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pre-trained word2vec\n",
    "pre_word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(datapath(os.path.join(EXT_DATA_FOLDER, \"GoogleNews-vectors-negative300.bin\")), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'woman' and 'man':  0.76640123\n",
      "Similarity of 'woman' and 'woman':  1.0\n",
      "Similarity of 'dog' and 'hotdog':  0.38931656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity of 'woman' and 'man': \", pre_word2vec_model.similarity('woman', 'man'))\n",
    "print(\"Similarity of 'woman' and 'woman': \", pre_word2vec_model.similarity('woman', 'woman'))\n",
    "print(\"Similarity of 'dog' and 'hotdog': \", pre_word2vec_model.similarity('dog', 'hotdog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in csv and excel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names for article_corpus:  ['URL' 'HTML' 'TEXT']\n",
      "==========\n",
      "Column names for annotated_corpus:  ['Identifier' 'Type' 'Category' 'URL' 'Cat 1' 'Cat2' 'Cat3' 'Cat4' 'Cat5'\n",
      " 'Cat6' 'Cat7' 'Score' 'First date_time' 'Tweets' 'Likes' 'Retweets'\n",
      " 'Potential exposure' 'Comment']\n",
      "(67, 18)\n"
     ]
    }
   ],
   "source": [
    "#Importing the csv and excel files\n",
    "article_corpus = pd.read_csv(os.path.join(EXT_DATA_FOLDER, \"url_text.csv\"))\n",
    "annotated_corpus = pd.read_excel(os.path.join(EXT_DATA_FOLDER, \"Credibility_Analysis_Samples\\\\September_13\\\\sample_third_adam_new.xlsx\"), header=1)\n",
    "article_corpus.columns = [\"URL\", \"HTML\", \"TEXT\"]\n",
    "print(\"Column names for article_corpus: \", article_corpus.columns.values)\n",
    "print(\"==========\")\n",
    "print(\"Column names for annotated_corpus: \", annotated_corpus.columns.values)\n",
    "\n",
    "annotated_articles = annotated_corpus.loc[(annotated_corpus[\"Cat7\"] == 0) | (annotated_corpus[\"Cat7\"] == 1)]\n",
    "print(annotated_articles.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 20)\n"
     ]
    }
   ],
   "source": [
    "from pandas import ExcelWriter\n",
    "\n",
    "#url_text.columns[2]\n",
    "title_text = article_corpus[\"TEXT\"][0].split(\"TEXT:\")\n",
    "#print(url_text[\"URL\"][0])\n",
    "#print(title_text[0].replace(\"TITLE:\",\"\").strip(\" \"))\n",
    "#print(title_text[1].strip(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(corpus_path, annotated_samples):\n",
    "    article_corpus = pd.read_csv(corpus_path)\n",
    "    annotated_corpus = pd.read_excel((annotated_samples), header=1)\n",
    "    article_corpus.columns = [\"URL\", \"HTML\", \"TEXT\"]\n",
    "    annotated_articles = annotated_corpus.loc[(annotated_corpus[\"Cat7\"] == 0) | (annotated_corpus[\"Cat7\"] == 1)]\n",
    "    dataset = annotated_articles.merge(article_corpus, how='left', on='URL')\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Identifier' 'Type' 'Category' 'URL' 'Cat 1' 'Cat2' 'Cat3' 'Cat4' 'Cat5'\n",
      " 'Cat6' 'Cat7' 'Score' 'First date_time' 'Tweets' 'Likes' 'Retweets'\n",
      " 'Potential exposure' 'Comment' 'HTML' 'TEXT']\n"
     ]
    }
   ],
   "source": [
    "corpus_path = os.path.join(EXT_DATA_FOLDER, \"url_text.csv\")\n",
    "annotated_path = os.path.join(EXT_DATA_FOLDER, \"Credibility_Analysis_Samples\\\\September_13\\\\sample_third_adam_new.xlsx\")\n",
    "dataset = create_dataset(corpus_path, annotated_path)\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://triblive.com/news/healthnow/12759008-74/stronger-flu-vaccine-for-elderly-could-help-younger-adults-with-chronic-conditions\n",
      "TITLE: Stronger flu vaccine for elderly could help younger adults with chronic conditions | TribLIVE\n",
      "TEXT:     “Persons who have these conditions have a much greater risk of the flu being more severe to the point of needing to be hospitalized,” Dr. Ken Smith, professor of medicine at Pitt and co-author of the paper, told the Tribune-Review on Thursday. “If you are hospitalized with the flu, your risk of dying is certainly something that is a possibility.”  The high-dose vaccine is recommended for the elderly population because their immune response to the standard-dose vaccine lessens as they age. However, the price for a standard dose is about $11, while the stronger vaccine is about $31 per dose, Smith said. He said the dose for the elderly is about 24 percent stronger than a standard vaccine.   “The growing proportion of middle-aged adults with chronic health conditions coupled with the modest effectiveness of the standard-dose influenza vaccine prompted us to explore whether existing vaccines already recommended for the elderly also could protect younger people,” lead author Jonathan Raviotta, senior research specialist with The Pittsburgh Vaccination Research Group (PittVax) in Pitt's School of Medicine, said in a statement. “Sure enough, expanding the recommendation does seem like a good policy. ... Before making such a recommendation, real world clinical trials are needed.”          \n"
     ]
    }
   ],
   "source": [
    "#Example of article with missing text\n",
    "print(dataset.head()[\"URL\"][3])\n",
    "print(dataset.head()[\"TEXT\"][3])\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataset locally\n",
    "writer = pd.ExcelWriter(\"dataset.xlsx\")\n",
    "dataset.to_excel(writer, \"Sheet1\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No text article found for the following url: \n",
      " http://vaccineinjurynews.com/2017-08-26-the-national-meningitis-association-are-a-front-for-the-vaccine-industry.html\n",
      "66\n",
      "Scientists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset2 = dataset\n",
    "count = 0\n",
    "cat4_scores = []\n",
    "art_text = []\n",
    "\n",
    "for index, row in dataset2.iterrows():\n",
    "    title_text = str(row[\"TEXT\"])\n",
    "    if(title_text != 'nan'):\n",
    "        count = count + 1\n",
    "        art_text.append(title_text.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \"))\n",
    "        cat4_scores.append(int(row[\"Cat7\"]))\n",
    "    else:\n",
    "        print(\"No text article found for the following url: \\n\", row[\"URL\"])\n",
    "print(count)  \n",
    "\n",
    "cat4_scores = np.array(cat4_scores)\n",
    "art_text = np.array(art_text)\n",
    "art_text_sent = np.array([sent_tokenize(article) for article in art_text])\n",
    "art_text_word = np.array([[word_tokenize(sent) for sent in article] for article in art_text_sent])\n",
    "#art_text_sent_word = np.array([[word_tokenize(sent) for sent in art_text_sent] for art in art_text_sent])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training set size ====\n",
      "# of articles in testing set: 52\n",
      "Number of articles that satisfy this category (=1): 13\n",
      "\n",
      "===== Testing set size ====\n",
      "# of articles in testing set: 14\n",
      "Number of articles that satisfy this category (=1): 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dividing into training and testing set\n",
    "\n",
    "#merge text and scores\n",
    "cat4_dataset = np.array(list(zip(art_text, cat4_scores)))\n",
    "\n",
    "#TODO: split this dataset into training and testing and then pass these into the next cell\n",
    "#training_set = cat4_dataset[:int(len(cat4_dataset)*0.8)]\n",
    "#testing_set = cat4_dataset[int(len(cat4_dataset)*0.8):]\n",
    "\n",
    "split = 0.8\n",
    "training_articles = art_text[:int(len(art_text)*split)]\n",
    "training_preds = cat4_scores[:int(len(cat4_scores)*split)]\n",
    "\n",
    "testing_articles = art_text[int(len(art_text)*split):]\n",
    "testing_preds = cat4_scores[int(len(cat4_scores)*split):]\n",
    "print(\"===== Training set size ====\")\n",
    "print(\"# of articles in testing set: {}\".format(len(training_preds)))\n",
    "print(\"Number of articles that satisfy this category (=1): {}\\n\".format(len(training_preds[training_preds == 1])))\n",
    "\n",
    "print(\"===== Testing set size ====\")\n",
    "print(\"# of articles in testing set: {}\".format(len(testing_preds)))\n",
    "print(\"Number of articles that satisfy this category (=1): {}\\n\".format(len(testing_preds[testing_preds == 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 52\n",
      "Naive Bayes correct prediction: 0.93\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Satisfies       0.93      1.00      0.96        13\n",
      "    Satisfies       0.00      0.00      0.00         1\n",
      "\n",
      "  avg / total       0.86      0.93      0.89        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM correct prediction: 0.93\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Satisfies       0.93      1.00      0.96        13\n",
      "    Satisfies       0.00      0.00      0.00         1\n",
      "\n",
      "  avg / total       0.86      0.93      0.89        14\n",
      "\n",
      "[[13  0]\n",
      " [ 1  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "categories = ['Not Satisfies', 'Satisfies']\n",
    "\n",
    "print(\"Number of articles: {}\".format(len(training_articles)))\n",
    "\n",
    "docs_test = testing_articles\n",
    "\n",
    "# Naive Bayes classifier\n",
    "bayes_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())\n",
    "                      ])\n",
    "bayes_clf.fit(training_articles, training_preds)\n",
    "joblib.dump(bayes_clf, \"naive_bayes.pkl\", compress=9)\n",
    "\n",
    "# Predict the test dataset using Naive Bayes\n",
    "predicted = bayes_clf.predict(docs_test)\n",
    "print('Naive Bayes correct prediction: {:4.2f}'.format(np.mean(predicted == testing_preds)))\n",
    "print(metrics.classification_report(testing_preds, predicted, target_names=categories))\n",
    "\n",
    "# Support Vector Machine (SVM) classifier\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=   5, random_state=42)),\n",
    "])\n",
    "svm_clf.fit(training_articles, training_preds)\n",
    "joblib.dump(svm_clf, \"svm.pkl\", compress=9)\n",
    "# Predict the test dataset using SVM\n",
    "predicted = svm_clf.predict(docs_test)\n",
    "print('SVM correct prediction: {:4.2f}'.format(np.mean(predicted == testing_preds)))\n",
    "print(metrics.classification_report(testing_preds, predicted, target_names=categories))\n",
    "\n",
    "print(metrics.confusion_matrix(testing_preds, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of twent_test.data:  <class 'numpy.ndarray'>\n",
      "66\n",
      "[2 2 2 ... 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"type of twent_test.data: \", type(twenty_train.target))\n",
    "print(len(art_text))\n",
    "print(twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
