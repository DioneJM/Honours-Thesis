{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt') #uncomment if running on new machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Helpful variables\n",
    "EXT_DATA_FOLDER = \"C:\\\\Users\\\\Admin\\\\Projects\\\\thesis\\\\data\\\\\"\n",
    "EXT_DATA_FOLDER2 = \"B:\\\\Datasets\\\\\"\n",
    "\n",
    "ANALYSIS_SAMPLES = os.path.join(EXT_DATA_FOLDER, \"Credibility_Analysis_Samples\\\\September_25\\\\\")\n",
    "dataset_columns = ['Identifier', 'Type', 'Category', 'URL', 'Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5',\n",
    " 'Cat6', 'Cat7', 'Score', 'First date_time', 'Tweets', 'Likes', 'Retweets',\n",
    " 'Potential exposure', 'HTML', 'TEXT']\n",
    "criterias = [\"Cat1\", \"Cat2\", \"Cat3\", \"Cat4\", \"Cat5\", \"Cat6\", \"Cat7\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5', 'Cat6', 'Cat7', 'Category',\n",
       "       'First date_time', 'HTML', 'Identifier', 'Likes', 'Potential Exposure',\n",
       "       'Potential exposure', 'Retweets', 'Score', 'TEXT', 'Tweets', 'Type',\n",
       "       'URL', 'Unnamed: 0', 'Low Credibility'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_articles.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_text_sent = np.array([sent_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_word = np.array([word_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_sent_word = np.array([[word_tokenize(sent) for sent in article] for article in art_text_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Classifier Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for criteria in criterias:\n",
    "    print(\"Training Classifier for \" + criteria + \":\")\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(labelled_articles[\"TEXT\"]), list(labelled_articles[criteria]), test_size=int(20))\n",
    "\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(max_features=10)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               maxj_iter=5, tol=None)),\n",
    "    ])\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_predicted = list(svm_clf.predict(X_test))\n",
    "    svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "\n",
    "    print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "    joblib.dump(svm_clf, 'models/svm_clf_tfidf_stop_' + criteria + '.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Credibility Score Detector\n",
    "\n",
    "1) Create a dataset that separates he articles as having low credibility and not low credibility where low credibility is defined to be articles with a credibility score of less than 3\n",
    "\n",
    "2.a) Apply the ensemble of models to classify an article for each category and then use the resulting score to determine whether an article's credibility is low or not\n",
    "\n",
    "2.b) Apply a new model that has been trained to solely just determine whether an article's credibility is low or not\n",
    "\n",
    "3) ???\n",
    "\n",
    "4) Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the manually labelled articles from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470, 21)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "labelled_articles = pd.read_excel(\"dataset5.xlsx\")\n",
    "labelled_articles = labelled_articles.dropna(subset=['TEXT'])\n",
    "labelled_articles = labelled_articles[pd.to_numeric(labelled_articles['Cat1'], errors='coerce').notnull()]\n",
    "for criteria in criterias:\n",
    "    labelled_articles = labelled_articles.dropna(subset=[criteria])\n",
    "print(labelled_articles.shape)\n",
    "labelled_articles['Low Credibility'] = np.where(labelled_articles['Score'] < 3, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Training and testing sets for 2 classification tasks\n",
    "\n",
    "1) Make a training and testing set for classfying whether an article has low credibility or not\n",
    "\n",
    "2) Make a training and testing set for classifying whether an article satisfies each criteria and then using these outputs do determine whether the article has low crediibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = labelled_articles[\"TEXT\"]\n",
    "crit1_labels = labelled_articles[\"Cat1\"]\n",
    "crit2_labels = labelled_articles[\"Cat2\"]\n",
    "crit3_labels = labelled_articles[\"Cat3\"]\n",
    "crit4_labels = labelled_articles[\"Cat4\"]\n",
    "crit5_labels = labelled_articles[\"Cat5\"]\n",
    "crit6_labels = labelled_articles[\"Cat6\"]\n",
    "crit7_labels = labelled_articles[\"Cat7\"]\n",
    "low_cred_labels = labelled_articles[\"Low Credibility\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(labelled_articles, labelled_articles, test_size=0.10)\n",
    "\n",
    "#Training and testing sets for classifying an article as low credibility or not\n",
    "X_cred_train, X_cred_test, y_cred_train, y_cred_test = train_test_split(article_text, low_cred_labels, test_size=0.10)\n",
    "\n",
    "#Training and testing set for whether an article meets criteria 1\n",
    "X_c1_train, X_c1_test, y_c1_train, y_c1_test = train_test_split(article_text, crit1_labels, test_size=0.10)\n",
    "\n",
    "#Training and testing set for whether an article meets criteria 2\n",
    "X_c2_train, X_c2_test, y_c2_train, y_c2_test = train_test_split(article_text, crit2_labels, test_size=0.10)\n",
    "\n",
    "#Training and testing set for whether an article meets criteria 1\n",
    "X_c3_train, X_c3_test, y_c3_train, y_c3_test = train_test_split(article_text, crit3_labels, test_size=0.10)\n",
    "\n",
    "#Training and testing set for whether an article meets criteria 1\n",
    "X_c4_train, X_c4_test, y_c4_train, y_c4_test = train_test_split(article_text, crit4_labels, test_size=0.10)\n",
    "\n",
    "#Training and testing set for whether an article meets criteria 1\n",
    "X_c5_train, X_c5_test, y_c5_train, y_c5_test = train_test_split(article_text, crit5_labels, test_size=0.10)\n",
    "\n",
    "#Training and testing set for whether an article meets criteria 1\n",
    "X_c6_train, X_c6_test, y_c6_train, y_c6_test = train_test_split(article_text, crit6_labels, test_size=0.10)\n",
    "\n",
    "#Training and testing set for whether an article meets criteria 1\n",
    "X_c7_train, X_c7_test, y_c7_train, y_c7_test = train_test_split(article_text, crit7_labels, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Model Approach\n",
    "SVM + TF-IDF with stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train(article_text, labels, X_train, y_train):\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(max_features=10)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    return svm_clf\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "classifiers = []\n",
    "for criteria in criterias:\n",
    "    classifiers.append(build_train(article_text, labelled_articles[criteria]))\n",
    "\n",
    "c1_clf = build_train(article_text, )\n",
    "\n",
    "svm_predicted = list(svm_clf.predict(X_test))\n",
    "svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "\n",
    "print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "joblib.dump(svm_clf, 'models/svm_clf_tfidf_stop_' + criteria + '.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
