{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi label Baseline Tests\n",
    "This notebook includes the classifiers that will be used to determine the performance of the deep learning-based classifier for the multi label scenario.\n",
    "\n",
    "Multi label classification is being considered for the following reasons:\n",
    "\n",
    "- One of the main ways to perform a classification tasks\n",
    "\n",
    "- It is hypothesised that a multi label classifier will have a drastically lower amount of training time required (training 7 classifiers vs 1) for only a slight drop in the classifier's performance (a multi labelling task is more complex - but how much more complex can it be than a binary classification task?)\n",
    "\n",
    "### Classifiers:\n",
    "\n",
    "1) Naive Bayes\n",
    "\n",
    "2) Support Vector Machines (SVM)\n",
    "\n",
    "### Performance Measures:\n",
    "\n",
    "1) Storage requirements of the classifier and feature representation used\n",
    "\n",
    "2) Training time of the classifier\n",
    "\n",
    "3) Speed of the classifier\n",
    "\n",
    "4) Accuracy of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "# Helpful variables\n",
    "EXT_DATA_FOLDER = \"C:\\\\Users\\\\Admin\\\\Projects\\\\thesis\\\\data\\\\\"\n",
    "EXT_DATA_FOLDER2 = \"B:\\\\Datasets\\\\\"\n",
    "\n",
    "ANALYSIS_SAMPLES = os.path.join(EXT_DATA_FOLDER, \"Credibility_Analysis_Samples\\\\September_25\\\\\")\n",
    "dataset_columns = ['Identifier', 'Type', 'Category', 'URL', 'Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5',\n",
    " 'Cat6', 'Cat7', 'Score', 'First date_time', 'Tweets', 'Likes', 'Retweets',\n",
    " 'Potential exposure', 'HTML', 'TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "# nltk.download('punkt') #uncomment if running on new machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(corpus_path, annotated_samples):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    corpus_path: Path for a CSV file containing a list of article URLs and its article text\n",
    "    annotated_samples: Path of the excel file containing articles and its associated URL along with its labels\n",
    "    \n",
    "    Method:\n",
    "    Retrieves the article text by matching the URLs within the corpus_path and annotated_samples and creates a dataframe \n",
    "    containing the URL, article text and the article's corresponding labels.\n",
    "    \n",
    "    Output:\n",
    "    A pandas dataframe\n",
    "    \"\"\"\n",
    "    article_corpus = pd.read_csv(corpus_path)\n",
    "    annotated_corpus = pd.read_excel((annotated_samples))\n",
    "    article_corpus.columns = [\"URL\", \"HTML\", \"TEXT\"]\n",
    "    annotated_articles = annotated_corpus.loc[(annotated_corpus[\"Cat7\"] == 0) | (annotated_corpus[\"Cat7\"] == 1)]\n",
    "    dataset = pd.merge(annotated_articles, article_corpus, how='left', on='URL')\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = os.path.join(EXT_DATA_FOLDER, \"url_text.csv\")\n",
    "excel_files = [\"sample_third_adam_new.xlsx\", \"sample_third_amalie_new.xlsx\", \"sample_third_maryke_new.xlsx\"]\n",
    "\n",
    "df_files = []\n",
    "\n",
    "for filename in excel_files:\n",
    "    annotated_path = os.path.join(ANALYSIS_SAMPLES, filename)\n",
    "    data = create_dataset(corpus_path, annotated_path)\n",
    "    df_files.append(data)\n",
    "    \n",
    "dataset = pd.concat(df_files)\n",
    "\n",
    "print(dataset.columns.values)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataset locally\n",
    "writer = pd.ExcelWriter(\"multi_dataset.xlsx\")\n",
    "dataset.to_excel(writer, \"Sheet1\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 19)\n"
     ]
    }
   ],
   "source": [
    "#pre-processing\n",
    "from collections import defaultdict\n",
    "\n",
    "labelled_articles = pd.read_excel(\"dataset3.xlsx\")\n",
    "labelled_articles = labelled_articles.dropna(subset=['TEXT'])\n",
    "print(labelled_articles.shape)\n",
    "criterias = [\"Cat1\", \"Cat2\", \"Cat3\", \"Cat4\", \"Cat5\", \"Cat6\", \"Cat7\"]\n",
    "art_text_sent = np.array([sent_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_word = np.array([word_tokenize(article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \")) for article in labelled_articles[\"TEXT\"]])\n",
    "art_text_sent_word = np.array([[word_tokenize(sent) for sent in article] for article in art_text_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "labels = [labelled_articles[\"Cat1\"], labelled_articles[\"Cat2\"], labelled_articles[\"Cat3\"], labelled_articles[\"Cat4\"], labelled_articles[\"Cat5\"], labelled_articles[\"Cat6\"], labelled_articles[\"Cat7\"]]\n",
    "labels = np.array(labels).transpose()\n",
    "multi_labels = [[int(x) for x in row] for row in labels]\n",
    "print(multi_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "categories = ['Not Satisfied', 'Satisfied']\n",
    "nb_bow = []\n",
    "nb_tfidf = []\n",
    "nb_w2v = []\n",
    "svm_bow = []\n",
    "svm_tfidf = []\n",
    "svm_w2v = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
