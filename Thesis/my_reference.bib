% This file was created with JabRef 2.3.1.
% Encoding: Cp1252

@techreport{Vosoughi,
	abstract = {We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise {\~{}}126,000 stories tweeted by {\~{}}3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98{\%} agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.},
	author = {Vosoughi, Soroush and Roy, Deb and Aral, Sinan},
	booktitle = {Science},
	doi = {10.1126/science.aap9559},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vosoughi, Roy, Aral - Unknown - The spread of true and false news online.pdf:pdf},
	isbn = {0036-8075},
	issn = {0036-8075},
	mendeley-groups = {AIHI Thesis/Social Impact},
	number = {6380},
	pages = {1146--1151},
	pmid = {29590045},
	title = {{SI:The spread of true and false news online}},
	url = {http://science.sciencemag.org/content/sci/359/6380/1146.full.pdf{\%}0Ahttp://www.sciencemag.org/lookup/doi/10.1126/science.aap9559},
	volume = {359},
	year = {2018}
}

@article{Sommariva2018,
	abstract = {ABSTRACTBackground: The importance of social networking sites (SNSs) as platforms to engage in the correction of “fake news” has been documented widely. More evidence is needed to understand the popularity of health-related rumors and how Health Educators can optimize their use of SNSs. Purpose: The purpose of this study was to explore the spread of health rumors and verified information on SNSs using the Zika virus as a case study. Methods: A content analysis of Zika-related news stories on SNSs between February 2016 and January 2017 was conducted to verify accuracy (phase 1). Phase 1 was followed by an analysis of volume of shares (phase 2) and a thematic analysis of headlines (phase 3). Results: Rumors had three times more shares than verified stories. Popular rumors portray Zika as a conspiracy against the public and a low-risk issue and connect it to the use of pesticides. Discussion: This study identifies the value of integrating in-depth analysis of popular health-related rumors into the developmen...},
	author = {Sommariva, Silvia and Vamos, Cheryl and Mantzarlis, Alexios Dao, Lillie Uyen-Loan and {Martinez Tyson}, Dinorah},
	doi = {10.1080/19325037.2018.1473178},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sommariva et al. - 2018 - Spreading the (Fake) News Exploring Health Messages on Social Media and the Implications for Health Profession.pdf:pdf},
	issn = {1932-5037},
	journal = {American Journal of Health Education},
	mendeley-groups = {AIHI Thesis,AIHI Thesis/Social Impact},
	month = {jul},
	number = {4},
	pages = {246--255},
	publisher = {Routledge},
	title = {{Spreading the (Fake) News: Exploring Health Messages on Social Media and the Implications for Health Professionals Using a Case Study}},
	url = {https://www.tandfonline.com/doi/full/10.1080/19325037.2018.1473178},
	volume = {49},
	year = {2018}
}

@book{Goodfellow-et-al-2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@misc{germanFN,
	booktitle = {The Guardian},
	key = {The Guardian},
	mendeley-groups = {AIHI Thesis/Social Impact},
	title = {{Germany investigating unprecedented spread of fake news online | World news | The Guardian}},
	url = {https://www.theguardian.com/world/2017/jan/09/germany-investigating-spread-fake-news-online-russia-election},
	urldate = {2018-08-03},
	year = {2017}
}

@article{Conneau2017,
	abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VD-CNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.},
	archivePrefix = {arXiv},

	arxivId = {arXiv:1606.01781v2},
	author = {Conneau, Alexis and Schwenk, Holger and {Le Cun}, Yann and {Loıc Barrault}, Loıc},
	eprint = {arXiv:1606.01781v2},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Conneau et al. - 2017 - Very Deep Convolutional Networks for Text Classification.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Very Deep Convolutional Networks for Text Classification}},
	url = {https://arxiv.org/pdf/1606.01781.pdf},
	year = {2017}
}

@article{Zhang,
	abstract = {This article offers an empirical exploration on the use of character-level convolu-tional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1502.01710},
	author = {Zhang, Xiang and Zhao, Junbo and Lecun, Yann},
	eprint = {arXiv:1502.01710},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhao, Lecun - Unknown - Character-level Convolutional Networks for Text Classification(2).pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Character-level Convolutional Networks for Text Classification}},
	url = {https://arxiv.org/pdf/1509.01626.pdf},
	year = {2015}
}

@article{Burgess2006,
	abstract = {Background: The report of an hypothesised link between measles-mumps-rubella (MMR) vaccination and autism in 1998 became a major public health issue in the United Kingdom (UK), leaving most experts surprised by the overwhelming influence it had on public opinion about MMR vaccination. Coverage rates fell dramatically, and did not start to recover until 2004. Could this public reaction have been predicted? Methods: We used Sandman's model of components predicting community outrage to assess the MMR controversy. Results: The controversy fulfilled all of Sandman's 12 primary components and six of the eight additional components. Conclusions: The Sandman model provided a useful framework to analyse this controversy and explained a significant portion of the community reaction and subsequent fall in vaccination coverage rates.},
	author = {Burgess, David C and Burgess, Margaret A and Leask, Julie},
	doi = {10.1016/j.vaccine.2006.02.033},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burgess, Burgess, Leask - 2006 - The MMR vaccination and autism controversy in United Kingdom 1998-2005 Inevitable community outrage or.pdf:pdf},
	journal = {Vaccine},
	keywords = {Autism,Measles mumps rubella,Risk communication,Vaccination},
	mendeley-groups = {AIHI Thesis/Social Impact},
	pages = {3921--3928},
	title = {{The MMR vaccination and autism controversy in United Kingdom 1998-2005: Inevitable community outrage or a failure of risk communication?}},
	url = {https://ac.els-cdn.com/S0264410X06002076/1-s2.0-S0264410X06002076-main.pdf?{\_}tid=46d1dda6-f576-4f5e-ad53-d550f1cd9990{\&}acdnat=1534726962{\_}1b237371d8bb916694f34f0f951c84bc},
	volume = {24},
	year = {2006}
}

@article{Hasan2011,
	abstract = {This paper reviewed the most recent evaluation criteria methods which were used in different e-business services. Furthermore, it proposes general criteria for evaluating the quality of any website regardless of the type of service that it offers. The dimensions of the criteria are content quality, design quality, organization quality, and user-friendly quality. These dimensions together with their comprehensive indicators and check list can be used by web designers and developers to create quality websites to improve the electronic service and then the image of any organization on the Internet.},
	author = {Hasan, Layla and Abuelrub, Emad},
	doi = {10.1016/J.ACI.2009.03.001},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasan, Abuelrub - 2011 - Assessing the quality of web sites.pdf:pdf},
	issn = {2210-8327},
	journal = {Applied Computing and Informatics},
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	month = {jan},
	number = {1},
	pages = {11--29},
	publisher = {Elsevier},
	title = {{Assessing the quality of web sites}},
	url = {https://www.sciencedirect.com/science/article/pii/S2210832710000037},
	volume = {9},
	year = {2011}
}

@misc{HealthNewsReview,
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	title = {{Our Review Criteria - HealthNewsReview.org}},
	url = {https://www.healthnewsreview.org/about-us/review-criteria/},
	urldate = {2018-08-25}
}


@misc{DISCERN,
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	title = {{DISCERN - The DISCERN Instrument}},
	url = {http://www.discern.org.uk/discern{\_}instrument.php},
	urldate = {2018-08-26}
}

@article{QIMR,
	abstract = {The media serves as an important link between medical research, as reported in scholarly sources, and the public and has the potential to act as a powerful tool to improve public health. However, concerns about the reliability of health research reports have been raised. Tools to monitor the quality of health research reporting in the media are needed to identify areas of weakness in health research reporting and to subsequently work towards the efficient use of the lay media as a public health tool through which the public's health behaviors can be improved. We developed the Quality Index for health-related Media Reports (QIMR) as a tool to monitor the quality of health research reports in the lay media. The tool was developed according to themes generated from interviews with health journalists and researchers. Item and domain characteristics and scale reliability were assessed. The scale was correlated with a global quality assessment score and media report word count to provide evidence towards its construct validity. The items and domains of the QIMR demonstrated acceptable validity and reliability. Items from the ‘validity' domain were negatively skewed, suggesting possible floor effect. These items were not eliminated due to acceptable content and face validity. QIMR total scores produced a strong correlation with raters' global assessment and a moderate correlation with media report word count, providing evidence towards the construct validity of the instrument. The results of this investigation indicate that QIMR can adequately measure the quality of health research reports, with acceptable reliability and validity.},
	author = {Zeraatkar, Dena and Obeda, Michael and Ginsberg, Jeffrey S. and Hirsh, Jack},
	doi = {10.1186/s12889-017-4259-y},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeraatkar et al. - 2017 - The development and validation of an instrument to measure the quality of health research reports in the lay m.pdf:pdf},
	issn = {1471-2458},
	journal = {BMC Public Health},
	keywords = {Biostatistics,Environmental Health,Epidemiology,Medicine/Public Health,Public Health,Vaccine,general},
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	month = {dec},
	number = {1},
	pages = {343},
	publisher = {BioMed Central},
	title = {{The development and validation of an instrument to measure the quality of health research reports in the lay media}},
	url = {http://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-017-4259-y},
	volume = {17},
	year = {2017}
}

@article{Pasupa2016,
	author = {Pasupa, Kitsuchart},
	file = {:C$\backslash$:/Users/Admin/Downloads/07863293.pdf:pdf},
	isbn = {9781509041398},
	keywords = {deep learning,machine learning,shallow learning},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{A Comparison between Shallow and Deep Architecture Classifiers on Small Dataset}},
	year = {2016}
}

@article{Aggarwal2012,
	abstract = {The problem of classification has been widely studied in the data mining, machine learning, database, and information retrieval communities with applications in a number of diverse domains, such as target marketing, medical diagnosis, news group filtering, and document organization. In this paper we will provide a survey of a wide variety of text classification algorithms.},
	author = {Aggarwal, Charu C and Zhai, Chengxiang},
	doi = {10.1007/978-1-4614-3223-4_6},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aggarwal, Zhai - 2012 - A SURVEY OF TEXT CLASSIFICATION ALGORITHMS.pdf:pdf},
	
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{A SURVEY OF TEXT CLASSIFICATION ALGORITHMS}},
	url = {http://alias-i.com/lingpipe/},
	year = {2012}
}

@article{Korde2012,
	abstract = {As most information (over 80{\%}) is stored as text, text mining is believed to have a high commercial potential value. knowledge may be discovered from many sources of information; yet, unstructured texts remain the largest readily available source of knowledge .Text classification which classifies the documents according to predefined categories .In this paper we are tried to give the introduction of text classification, process of text classification as well as the overview of the classifiers and tried to compare the some existing classifier on basis of few criteria like time complexity, principal and performance.},
	author = {Korde, Vandana and Mahender, C Namrata},
	doi = {10.5121/ijaia.2012.3208},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korde, Mahender - 2012 - TEXT CLASSIFICATION AND CLASSIFIERS A SURVEY.pdf:pdf},
	journal = {International Journal of Artificial Intelligence {\&} Applications (IJAIA)},
	keywords = {Classifiers,Text Representation,Text classification},
	mendeley-groups = {AIHI Thesis/Document Classification},
	number = {2},
	title = {{TEXT CLASSIFICATION AND CLASSIFIERS: A SURVEY}},
	url = {http://aircconline.com/ijaia/V3N2/3212ijaia08.pdf},
	volume = {3},
	year = {2012}
}

@techreport{Allahyari2017,
	abstract = {The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore , efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1707.02919v2},
	author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saied and Trippe, Elizabeth D and Gutierrez, Juan B and Kochut, Krys},
	eprint = {arXiv:1707.02919v2},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Allahyari et al. - 2017 - A Brief Survey of Text Mining Classification, Clustering and Extraction Techniques.pdf:pdf},
	keywords = {CCS CONCEPTS • Information systems → Document topi,Clustering and classification,Informa-tion extraction,KEYWORDS Text mining, classification, clustering,},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques}},
	url = {http://en.wikipedia.org/wiki/Statistics},
	volume = {13},
	year = {2017}
}


@techreport{Sahami,
	abstract = {In addressing the growing problem of junk email on the Internet, we examine methods for the automated construction of filters to eliminate such unwanted messages from a user's mail stream. By casting this problem in a decision theoretic framework, we are able to make use of probabilistic learning methods in conjunction with a notion of differential misclassification cost to produce filters which are especially appropriate for the nuances of this task. While this may appear, at first, to be a straightforward text classification problem, we show that by considering domain-specic features of this problem, in addition to the raw text of E-mail messages, we can produce much more accurate filters. Finally, we show the efficacy of such filters in a real world usage scenario, arguing that this technology is mature enough for deployment.},
	author = {Sahami, Mehran and Dumais, Susan and Heckerman, David and Horvitz, Eric},
	booktitle = {Learning for Text Categorization: Papers from the AAAI Workshop},
	doi = {10.1.1.48.1254},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahami et al. - Unknown - A Bayesian Approach to Filtering Junk E-Mail.pdf:pdf},
	isbn = {9781612507866},
	issn = {09258388},
	mendeley-groups = {AIHI Thesis/Document Classification},
	number = {Cohen},
	pages = {55--62},
	title = {{A Bayesian approach to filtering junk e-mail}},
	url = {http://research.microsoft.com/en-us/um/people/horvitz/junkfilter.htm},
	volume = {WS-98-05},
	year = {1998}
}

@techreport{McCallum1998,
	abstract = {Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes—providing on average a 27{\%} reduction in error over the multi-variate Bernoulli model at any vocabulary size. Introduction},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {McCallum, Andres and Nigam, Kamal},
	booktitle = {AAAI/ICML-98 Workshop on Learning for Text Categorization},
	doi = {10.1.1.46.1529},
	eprint = {0-387-31073-8},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McCallum, Nigam - 1998 - A Comparison of Event Models for Naive Bayes Text Classification.pdf:pdf},
	isbn = {0897915240},
	issn = {0343-6993},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {41--48},
	pmid = {20236947},
	title = {{A Comparison of Event Models for Naive Bayes Text Classification}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.9324{\&}rep=rep1{\&}type=pdf},
	year = {1998}
}

@techreport{Informatik1997,
	author = {Informatik, Fachbereich},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Informatik - 1997 - UNIVERSIT AT D ORTMUND Text Categorization with Support Vector Machines Learning with Many Relevant F eatures Thorst.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{UNIVERSIT AT D ORTMUND Text Categorization with Support Vector Machines: Learning with Many Relevant F eatures Thorsten Joachims}},
	url = {https://www.cs.cornell.edu/people/tj/publications/joachims{\_}97b.pdf},
	year = {1997}
}

@techreport{Basheer2000,
	abstract = {Artificial neural networks (ANNs) are relatively new computational tools that have found extensive utilization in solving many complex real-world problems. The attractiveness of ANNs comes from their remarkable information processing characteristics pertinent mainly to nonlinearity, high parallelism, fault and noise tolerance, and learning and generalization capabilities. This paper aims to familiarize the reader with ANN-based computing (neurocomputing) and to serve as a useful companion practical guide and toolkit for the ANNs modeler along the course of ANN project development. The history of the evolution of neurocomputing and its relation to the field of neurobiology is briefly discussed. ANNs are compared to both expert systems and statistical regression and their advantages and limitations are outlined. A bird's eye review of the various types of ANNs and the related learning rules is presented, with special emphasis on backpropagation (BP) ANNs theory and design. A generalized methodology for developing successful ANNs projects from conceptualization, to design, to implementation, is described. The most common problems that BPANNs developers face during training are summarized in conjunction with possible causes and remedies. Finally, as a practical application, BPANNs were used to model the microbial growth curves of S. flexneri. The developed model was reasonably accurate in simulating both training and test time-dependent growth curves as affected by temperature and pH.},
	author = {Basheer, I A and Hajmeer, M},
	booktitle = {Journal of Methods Microbiological Journal of Microbiological Methods},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Basheer, Hajmeer - 2000 - Artificial neural networks fundamentals, computing, design, and application(2).pdf:pdf},
	keywords = {Artificial neural networks,Backpropagation,Growth curves,History,Modeling,S flexneri},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {3--31},
	title = {{Artificial neural networks: fundamentals, computing, design, and application}},
	url = {www.elsevier.com/locate/jmicmeth},
	volume = {43},
	year = {2000}
}


@article{aizerman67theoretical,
	added-at = {2008-03-10T11:08:35.000+0100},
	author = {Aizerman, M. A. and Braverman, E. A. and Rozonoer, L.},
	biburl = {https://www.bibsonomy.org/bibtex/285cd047c112c39dabab7ecd47c00cf01/sb3000},
	interhash = {4848b39ce0f837dc7b429b700526c5b1},
	intrahash = {85cd047c112c39dabab7ecd47c00cf01},
	journal = {Automation and Remote Control},
	keywords = {kernel},
	month = {June},
	pages = {821-837},
	timestamp = {2010-10-07T14:13:58.000+0200},
	title = {Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning},
	volume = 25,
	year = 1964
}

@misc{UnderstandingKernel,
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Understanding the kernel trick. – Towards Data Science}},
	url = {https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78},
	urldate = {2018-08-31}
}

@techreport{Rosenfeld2000,
	abstract = {Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the first significant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them here, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data.},
	author = {Rosenfeld, Ronald},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenfeld - Unknown - TWO DECADES OF STATISTICAL LANGUAGE MODELING WHERE DO WE GO FROM HERE(2).pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{TWO DECADES OF STATISTICAL LANGUAGE MODELING: WHERE DO WE GO FROM HERE?}},
	url = {https://www.cs.cmu.edu/{~}roni/papers/survey-slm-IEEE-PROC-0004.pdf},
	year = {2000}
}

@article{Zhou2015,
	abstract = {Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document mod-eling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations , and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1511.08630v2},
	author = {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C M},
	eprint = {arXiv:1511.08630v2},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2015 - A C-LSTM Neural Network for Text Classification.pdf:pdf},
	keywords = {()},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{A C-LSTM Neural Network for Text Classification}},
	url = {https://arxiv.org/pdf/1511.08630.pdf},
	year = {2015}
}

@article{Hall2017,
	author = {Hall, Victoria and Banerjee, Emily and Kenyon, Cynthia and Strain, Anna and Griffith, Jayne and Como-Sabetti, Kathryn and Heath, Jennifer and Bahta, Lynn and Martin, Karen and McMahon, Melissa and Johnson, Dave and Roddy, Margaret and Dunn, Denise and Ehresmann, Kristen},
	doi = {10.15585/mmwr.mm6627a1},
	file = {:C$\backslash$:/Users/Admin/Downloads/mm6627a1.pdf:pdf},
	issn = {0149-2195},
	journal = {MMWR. Morbidity and Mortality Weekly Report},
	mendeley-groups = {AIHI Thesis/Social Impact},
	month = {jul},
	number = {27},
	pages = {713--717},
	title = {{Measles Outbreak — Minnesota April–May 2017}},
	url = {http://www.cdc.gov/mmwr/volumes/66/wr/mm6627a1.htm},
	volume = {66},
	year = {2017}
}

@article{Cipriani,
	abstract = {Purpose The review focused on the role that media reporting plays in the level of public awareness about osteoporosis and its influence on osteoporosis treatment decisions. Methods We reviewed the literature on the role of media on three main aspects influencing patient adherence to osteoporo-sis treatment: the awareness of osteoporosis as a major health problem, the perception of the effectiveness of osteoporosis medications, and the fear of adverse effects with osteoporosis medications. Results A review of the literature confirmed what is routinely observed in clinical practice-that media report can strongly influence the level of awareness of osteoporosis and fracture risk. Inadequate and/or incorrect information on osteoporosis in the media are associated with a low level of awareness of the disease. High-risk patients may have a poor understanding of the need for treatment. Alarming information in the media over the last 2 decades regarding effectiveness and safety of long-term osteoporosis treatment is associated with reduction in the use of osteoporosis medications. Conclusions There is a gap between the application of clinical recommendations and patient perceptions of osteoporosis and its treatment. There is a need for better education of patients and practitioners aimed at recognizing the serious consequences of fractures and understanding the expected benefits and potential risks of treatment. Media reports that disseminate evidence-based information on the balance of benefits and risks could help to reduce the osteoporosis treatment gap and mitigate the crisis in osteoporosis care.},
	author = {Cipriani, Cristiana and Pepe, Jessica and Minisola, Salvatore and Lewiecki, {\textperiodcentered} E Michael},
	doi = {10.1007/s40618-018-0898-9},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cipriani et al. - 2018 - Adverse effects of media reports on the treatment of osteoporosis.pdf:pdf},
	journal = {Journal of Endocrinological Investigation},
	keywords = {Fracture,Media,Osteoporosis,Treatment},
	title = {{Adverse effects of media reports on the treatment of osteoporosis}},
	url = {https://doi.org/10.1007/s40618-018-0898-9}
}

@techreport{Kaicker2010,
	abstract = {Background: The Internet is used increasingly by providers as a tool for disseminating pain-related health information and by patients as a resource about health conditions and treatment options. However, health information on the Internet remains unregulated and varies in quality, accuracy and readability. The objective of this study was to determine the quality of pain websites, and explain variability in quality and readability between pain websites. Methods: Five key terms (pain, chronic pain, back pain, arthritis, and fibromyalgia) were entered into the Google, Yahoo and MSN search engines. Websites were assessed using the DISCERN instrument as a quality index. Grade level readability ratings were assessed using the Flesch-Kincaid Readability Algorithm. Univariate (using alpha = 0.20) and multivariable regression (using alpha = 0.05) analyses were used to explain the variability in DISCERN scores and grade level readability using potential for commercial gain, health related seals of approval, language(s) and multimedia features as independent variables. Results: A total of 300 websites were assessed, 21 excluded in accordance with the exclusion criteria and 110 duplicate websites, leaving 161 unique sites. About 6.8{\%} (11/161 websites) of the websites offered patients' commercial products for their pain condition, 36.0{\%} (58/161 websites) had a health related seal of approval, 75.8{\%} (122/161 websites) presented information in English only and 40.4{\%} (65/161 websites) offered an interactive multimedia experience. In assessing the quality of the unique websites, of a maximum score of 80, the overall average DISCERN Score was 55.9 (13.6) and readability (grade level) of 10.9 (3.9). The multivariable regressions demonstrated that website seals of approval (P = 0.015) and potential for commercial gain (P = 0.189) were contributing factors to higher DISCERN scores, while seals of approval (P = 0.168) and interactive multimedia (P = 0.244) contributed to lower grade level readability, as indicated by estimates of the beta coefficients.},
	author = {Kaicker, Jatin and {Borg Debono}, Victoria and Dang, Wilfred and Buckley, Norman and Thabane, Lehana},
	doi = {10.1186/1741-7015-8-59},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaicker et al. - 2010 - Assessment of the quality and variability of health information on chronic pain websites using the DISCERN instr.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	title = {{Assessment of the quality and variability of health information on chronic pain websites using the DISCERN instrument}},
	url = {http://www.biomedcentral.com/1741-7015/8/59},
	year = {2010}
}

@article{Som2012,
	abstract = {Internet chemotherapy information is of good quality: assessment with the DISCERN tool},
	author = {Som, R and Gunawardana, N P},
	doi = {10.1038/bjc.2012.223},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Som, Gunawardana - 2012 - Internet chemotherapy information is of good quality assessment with the DISCERN tool.pdf:pdf},
	issn = {0007-0920},
	journal = {British Journal of Cancer},
	keywords = {Chemotherapy,Health occupations},
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	month = {jul},
	number = {2},
	pages = {403--403},
	publisher = {Nature Publishing Group},
	title = {{Internet chemotherapy information is of good quality: assessment with the DISCERN tool}},
	url = {http://www.nature.com/articles/bjc2012223},
	volume = {107},
	year = {2012}
}


@article{CanteyBanasiak2017,
	abstract = {Purpose: The primary purpose of this study was to examine the quality of sponsored and unsponsored asthma websites using the Brief DISCERN instrument and to evaluate whether the Health On the Net Code of Conduct (HONcode) logo was present, thereby indicating that the site met the criteria. The Internet is an important source of health information for patients and their families. The primary purpose of this study was to examine the quality of sponsored and unsupported asthma websites. A secondary aim was to determine the readability and reading ease of the materials for each website along with the grade level. Methods: We queried seven Internet search engines using the keyword "asthma." The websites were evaluated using the six-item Brief DISCERN instrument and by ascertaining whether the HONcode quality label was present. The websites were also evaluated for readability employing Flesch-Kincaid grade level and Flesch reading ease tools using Microsoft Office Word 2013 software. Results: A total of 22 unique websites were included in the study. Approximately 68{\%} of the websites reviewed had a Brief DISCERN cutoff score of ≥16. The overall Brief DISCERN scores ranged from 6 to 30, and the mean score was 17.32 (SD =6.71). The Flesch-Kincaid grade level scores ranged from 2.9 to 15.4, and the average reading grade score was 9.49 (SD =2.7). The Flesch reading ease scores ranged from 17 to 82.7, with a mean reading ease score of 53.57 (SD =15.03). Sites with a HONcode quality label had significantly higher Brief DISCERN scores than those without one (t=2.3795; df=20; p=0.02). Conclusion: Brief DISCERN scores revealed that there is quality asthma information for children and their families available on the Internet. The grade level ranged between 2.9 and 15.4 among the websites. However, the mean grade level scores were 9.3-9.89, which is high for the average consumer. Access to accurate information via the Internet, with appropriate readability, may enable pediatric asthma patients and their caregivers to better control and manage asthma.},
	author = {{Cantey Banasiak}, Nancy and Meadows-Oliver, Mikki},
	doi = {10.2147/JAA.S133536},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cantey Banasiak, Meadows-Oliver - 2017 - Journal of Asthma and Allergy Dovepress Evaluating asthma websites using the Brief DISCERN inst.pdf:pdf},
	journal = {Journal of Asthma and Allergy},
	keywords = {DISCERN,Internet,asthma,patient education},
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	pages = {10--191},
	title = {{Journal of Asthma and Allergy Dovepress Evaluating asthma websites using the Brief DISCERN instrument}},
	url = {http://dx.doi.org/10.2147/JAA.S133536},
	year = {2017}
}

@techreport{Batchelor2009,
	abstract = {Background: As patients share in the decision-making process regarding treatments they receive, it is important that they can discriminate between reliable and unreliable sources of information about potential treatments. Methods: In this study, health professionals and patients were asked to assess the reliability of information contained in pamphlets on treatments for asthma and atopic dermatitis using a new Japanese translation of an instrument called DISCERN. The scores given by both groups were analyzed to assess inter-rater agreement. The same DISCERN instrument was used by health professionals to evaluate websites on treatments for atopic dermatitis and the degree of inter-rater agreement was assessed again. Results: There was a greater inter-rater agreement between health professionals than between patients. When health professionals used the instrument to evaluate websites, the final rankings given were consistent between different raters, showing good inter-rater agreement. Conclusions: We conclude that DISCERN is useful for evaluating the reliability of medical information both in pamphlets and on the internet, although it is used more effectively by health professionals than by patients. Further studies are needed on the use of DISCERN by patients in evaluating websites containing medical information .},
	author = {Batchelor, Jonathan M and Ohya, Yukihiro},
	booktitle = {Allergology International},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Batchelor, Ohya - 2009 - Use of the DISCERN Instrument by Patients and Health Professionals to Assess Information Resources on Treatment.pdf:pdf},
	keywords = {evaluation studies,internet,medical informatics,pamphlets,patients},
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	title = {{Use of the DISCERN Instrument by Patients and Health Professionals to Assess Information Resources on Treatments for Asthma and Atopic Dermatitis}},
	url = {www.jsaweb.jp!},
	volume = {58},
	year = {2009}
}

@article{Matsoukas2008,
	abstract = {Based on criteria for assessing the quality of health information extracted from a review of the literature, we expanded the DISCERN instrument by adding 14 questions. New questions addressed language of the web-based health information resource, level of readability of the resource, and usability of the web page or portal for the resource.},
	author = {Matsoukas, Konstantina and Hyun, Sookyung and Currie, Leanne and Joyce, Myra P and Oliver, John and Patel, Sapana and Velez, Olivia and Yen, Po-Yin and Bakken, Suzanne},
	journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
	keywords = {Consumer Behavior,Consumer Health Information -- Classification,Internet,Quality Assurance, Health Care -- Methods,Quality Assurance, Health Care -- Standards,Software,Surveys and Questionnaires},
	mendeley-groups = {AIHI Thesis/Assessing Credibility},
	pages = {1048},
	title = {{Expanding DISCERN to create a tool for assessing the quality of Web-based health information resources}},
	year = {2008}
}

@techreport{Young,
	abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP. I. INTRODUCTION Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing, in which the analysis of a sentence could take up to 7 minutes, to the era of Google and the likes of it, in which millions of webpages can be processed in less than a second [1]. NLP enables computers to perform a wide range of natural language related tasks at all levels, ranging from parsing and part-of-speech (POS) tagging, to machine translation and dialogue systems. Deep learning architectures and algorithms have already made impressive advances in fields such as computer vision and pattern recognition. Following this trend, recent NLP research is now increasingly focusing on the use of new deep learning methods (see Figure 1). For decades, machine learning approaches targeting NLP problems have been based on shallow models (e.g., SVM and logistic regression) trained on very high dimensional and sparse features. In the last few years, neural networks based on dense vector representations have been producing superior results on various NLP tasks. This trend is sparked by the success of word embeddings [2, 3] and deep learning methods [4]. Deep learning enables multi-level automatic feature representation learning. In contrast, traditional machine learning based NLP systems liaise heavily on hand-crafted features. Such hand-crafted features are time-consuming and often incomplete. Collobert et al. [5] demonstrated that a simple deep learning framework outperforms most state-of-the-art approaches in several NLP tasks such as named-entity recognition (NER), semantic role labeling (SRL), and POS tagging. Since then, numerous complex deep learning based algorithms have been proposed to solve difficult NLP tasks. We review major deep learning related models and methods applied to natural language tasks such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and recursive neural networks. We also discuss memory-augmenting strategies, attention mechanisms and how unsupervised models, reinforcement learning methods and recently, deep generative models have been employed for language-related tasks. To the best of our knowledge, this work is the first of its type to comprehensively cover the most popular deep learning methods in NLP research today. The work by Goldberg [6] only presented the basic principles for applying neural networks to NLP in a tutorial manner. We believe this paper will give readers a more comprehensive idea of current practices in this domain. The structure of the paper is as follows: Section II introduces the concept of distributed representation, the basis of sophisticated deep learning models; next, Sections III, IV, and V discuss popular models such as convolutional, recurrent, and recursive neural networks, as well as their use in various NLP tasks; following, Section VI lists recent applications of reinforcement learning in NLP and new developments in unsupervised sentence representation learning; later, Section VII illustrates the recent trend of coupling deep learning models with memory modules; finally, Section VIII summarizes the performance of a series of deep learning methods on standard datasets about major NLP topics.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1708.02709v6},
	author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
	eprint = {arXiv:1708.02709v6},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Young et al. - Unknown - Recent Trends in Deep Learning Based Natural Language Processing.pdf:pdf},
	keywords = {Attention,Convolutional Neural Net-works,Deep Learning,Dialogue Systems,Index Terms Natural Language Processing,LSTM,Named-Entity Recognition,POS Tagging,Parsing,Question Answering,Recurrent Neural Networks,Semantic Role Labeling,Sentiment Analysis,Word2Vec},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Recent Trends in Deep Learning Based Natural Language Processing}},
	url = {http://veredshwartz.blogspot.sg.}
}

@techreport{Howard2018,
	abstract = {Inductive transfer learning has greatly im-pacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outper-forms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore , with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We open-source our pretrained models and code 1 .},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1801.06146v5},
	author = {Howard, Jeremy and Ruder, Sebastian},
	eprint = {arXiv:1801.06146v5},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Howard, Ruder - Unknown - Universal Language Model Fine-tuning for Text Classification.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Universal Language Model Fine-tuning for Text Classification}},
	url = {http://nlp.fast.ai/ulmfit.},
	year = {2018}
}

@article{Hochreiter1997,
	author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
	title = {Long Short-Term Memory},
	journal = {Neural Comput.},
	issue_date = {November 15, 1997},
	volume = {9},
	number = {8},
	month = nov,
	year = {1997},
	issn = {0899-7667},
	pages = {1735--1780},
	numpages = {46},
	url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	acmid = {1246450},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@techreport{Socher,
	abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model out-performs all previous methods on several met-rics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher et al. - Unknown - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {1631--1642},
	publisher = {Association for Computational Linguistics},
	title = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
	url = {http://nlp.stanford.edu/}
}

@article{Collobert2011,
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements .},
	author = {Collobert, Ronan and Weston, Jason and Com, Jweston@google and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
	journal = {Journal of Machine Learning Research},
	keywords = {natural language processing,neural networks},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {2493--2537},
	title = {{Natural Language Processing (Almost) from Scratch}},
	url = {http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf},
	volume = {12},
	year = {2011}
}

@techreport{Mikolov,
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Distributed Representations of Words and Phrases and their Compositionality}},
	url = {https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@techreport{Weston,
	abstract = {Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method, called WSABIE, both outperforms several baseline methods and is faster and consumes less memory.},
	author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weston, Bengio, Usunier - Unknown - WSABIE Scaling Up To Large Vocabulary Image Annotation.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{WSABIE: Scaling Up To Large Vocabulary Image Annotation}},
	url = {http://torch5.sourceforge.net/}
}

@techreport{Cambria2017,
	author = {Cambria, Erik and Poria, Soujanya and Gelbukh, Alexander and Nacional, Instituto Polit{\'{e}}cnico and Thelwall, Mike},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cambria et al. - 2017 - AFFECTIVE COMPUTING AND SENTIMENT ANALYSIS Sentiment Analysis Is a Big Suitcase.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{AFFECTIVE COMPUTING AND SENTIMENT ANALYSIS Sentiment Analysis Is a Big Suitcase}},
	url = {www.computer.org/intelligent},
	year = {2017}
}

@misc{RNNDiagram,
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs – WildML}},
	url = {http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/},
	urldate = {2018-09-06}
}

@techreport{Kim,
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1408.5882v2},
	author = {Kim, Yoon},
	eprint = {arXiv:1408.5882v2},
	file = {::},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Convolutional Neural Networks for Sentence Classification}},
	url = {http://nlp.stanford.edu/sentiment/}
}

@techreport{Nogueira,
	abstract = {Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that exploits from character-to sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: the Stanford Sentiment Tree-bank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7{\%} accuracy, and fine-grained classification, with 48.3{\%} accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4{\%}.},
	author = {Nogueira, C{\'{i}}cero and Santos, Dos and Gatti, Ma{\'{i}}ra},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Santos, Gatti - Unknown - Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {69--78},
	title = {{Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts}},
	url = {http://www.aclweb.org/anthology/C14-1008}
}


@incollection{LeCun1999,
	author = {LeCun, Yann and Haffner, Patrick and Bottou, L{\'{e}}on and Bengio, Yoshua},
	doi = {10.1007/3-540-46805-6_19},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1999 - Object Recognition with Gradient-Based Learning.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {319--345},
	publisher = {Springer, Berlin, Heidelberg},
	title = {{Object Recognition with Gradient-Based Learning}},
	url = {http://link.springer.com/10.1007/3-540-46805-6{\_}19},
	year = {1999}
}

@article{Pan2009,
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multi-task learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	doi = {10.1109/TKDE.2009.191},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Yang - 2009 - A Survey on Transfer Learning(2).pdf:pdf},
	keywords = {Data Mining,Index Terms-Transfer Learning,Machine Learning,Survey},
	mendeley-groups = {AIHI Thesis/Transfer Learning},
	title = {{A Survey on Transfer Learning}},
	url = {http://socrates.acadiau.ca/courses/comp/dsilver/NIPS95},
	year = {2009}
}

@article{Pan2009,
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multi-task learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	doi = {10.1109/TKDE.2009.191},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Yang - 2009 - A Survey on Transfer Learning(2).pdf:pdf},
	keywords = {Data Mining,Index Terms-Transfer Learning,Machine Learning,Survey},
	mendeley-groups = {AIHI Thesis/Transfer Learning},
	title = {{A Survey on Transfer Learning}},
	url = {http://socrates.acadiau.ca/courses/comp/dsilver/NIPS95},
	year = {2009}
}

@misc{Merity2016,
	abstract = {The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.},
	author = {Merity, Stephen},
	mendeley-groups = {AIHI Thesis/Transfer Learning},
	title = {{The wikitext long term dependency language modeling dataset}},
	url = {https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset},
	urldate = {2018-09-07},
	year = {2016}
}

@misc{Browniee2017,
	author = {Browniee, Jason},
	mendeley-groups = {AIHI Thesis/Transfer Learning},
	title = {{A Gentle Introduction to Transfer Learning for Deep Learning}},
	url = {https://machinelearningmastery.com/transfer-learning-for-deep-learning/},
	urldate = {2018-09-07},
	year = {2017}
}

@book{Bird2009,
	author = {Bird, Steven and Klein, Ewan and Loper, Edward},
	mendeley-groups = {AIHI Thesis/Document Classification},
	publisher = {O'Reilly Media Inc.},
	title = {{Natural Language Processing with Python}},
	url = {https://www.nltk.org/book/},
	year = {2009}
}

@article{Mikolov2013,
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	archivePrefix = {arXiv},
	arxivId = {1301.3781v3},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	eprint = {1301.3781v3},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Efficient Estimation of Word Representations in Vector Space(3).pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Efficient Estimation of Word Representations in Vector Space}},
	url = {http://ronan.collobert.com/senna/},
	year = {2013}
}

@misc{Googlew2v,
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Google Code Archive - word2vec}},
	url = {https://code.google.com/archive/p/word2vec/},
	urldate = {2018-10-30}
}

@techreport{Merity2016,
	abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction un-ambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus. 1},
	archivePrefix = {arXiv},
	arxivId = {1609.07843v1},
	author = {Merity, Stephen and Bradbury, James and Socher, Richard},
	eprint = {1609.07843v1},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merity, Bradbury, Socher - 2016 - Pointer Sentinel Mixture Models(2).pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Pointer Sentinel Mixture Models}},
	url = {https://arxiv.org/pdf/1609.07843.pdf},
	year = {2016}
}

@article{bradbury2016quasi,
	title={{Quasi-Recurrent Neural Networks}},
	author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
	mendeley-groups = {AIHI Thesis/Document Classification},
	journal={International Conference on Learning Representations (ICLR 2017)},
	year={2017}
}

@article{scikit-learn,
	title={{Scikit-learn: Machine Learning in Python }},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

@inproceedings{pennington2014glove,
	author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
	booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
	title = {GloVe: Global Vectors for Word Representation},
	year = {2014},
	pages = {1532--1543},
	url = {http://www.aclweb.org/anthology/D14-1162},
}

@misc{AWS,
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Amazon EC2 Instance Types – Amazon Web Services (AWS)}},
	url = {https://aws.amazon.com/ec2/instance-types/},
	urldate = {2018-11-01}
}

@inproceedings{paszke2017automatic,
	title={Automatic differentiation in PyTorch},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	booktitle={NIPS-W},
	year={2017}
}

@misc{AutismVaccine,
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{AUTISM {\&} VACCINES: IS THERE A LINK?}},
	url = {https://www.learntherisk.org/autism/},
	urldate = {2018-11-01}
}

@inproceedings{Lilleberg2015,
	author = {Lilleberg, Joseph and Zhu, Yun and Zhang, Yanqing},
	booktitle = {2015 IEEE 14th International Conference on Cognitive Informatics {\&} Cognitive Computing (ICCI*CC)},
	doi = {10.1109/ICCI-CC.2015.7259377},
	isbn = {978-1-4673-7290-9},
	mendeley-groups = {AIHI Thesis/Document Classification},
	month = {jul},
	pages = {136--140},
	publisher = {IEEE},
	title = {{Support vector machines and Word2vec for text classification with semantic features}},
	url = {http://ieeexplore.ieee.org/document/7259377/},
	year = {2015}
}

@misc{PRANCKEVICIUS2017,
	author = {PRANCKEVI{\v{C}}IUS, Tomas and Marcinkevicius, Virginijus},
	file = {:C$\backslash$:/Users/DM/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/PRANCKEVI{\v{C}}IUS, Marcinkevicius - 2017 - Comparison of Na{\"{i}}ve Bayes , Random Forest , Decision Tree , Support Vector Machines , and Logi(2).pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Comparison of Na{\"{i}}ve Bayes , Random Forest , Decision Tree , Support Vector Machines , and Logistic Regression Classifiers for Text Reviews Classification}},
	url = {https://www.semanticscholar.org/paper/Comparison-of-Na{\"{i}}ve-Bayes-{\%}2C-Random-Forest-{\%}2C-Tree-{\%}2C-PRANCKEVI{\v{C}}IUS-Marcinkevicius/e853f8827a2b9c596296de296b482e1175f9713f?navId=similar-papers},
	year = {2017}
}

@misc{Wang2012,
	author = {Wang, Sida and Manning, Christopher D.},
	booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2},
	file = {:C$\backslash$:/Users/DM/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Manning - 2012 - Baselines and bigrams simple, good sentiment and topic classification(2).pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {90--94},
	publisher = {Association for Computational Linguistics},
	title = {{Baselines and bigrams: simple, good sentiment and topic classification}},
	url = {https://dl.acm.org/citation.cfm?id=2390688},
	year = {2012}
}

@techreport{Pang2004,
	abstract = {Sentiment analysis seeks to identify the view-point(s) underlying a text span; an example application is classifying a movie review as "thumbs up" or "thumbs down". To determine this sentiment polarity , we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.},
	author = {Pang, Bo and Lee, Lillian},
	file = {:C$\backslash$:/Users/DM/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pang, Lee - 2004 - A Sentimental Education Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.pdf:pdf},
	keywords = {()},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts}},
	url = {www.cs.cornell.edu/people/pabo/movie-},
	year = {2004}
}

@techreport{Hassan,
	abstract = {The activity of labeling of documents according to their content is known as text categorization. Many experiments have been carried out to enhance text categorization by adding background knowledge to the document using knowledge repositories like Word Net, Open Project Directory (OPD), Wikipedia and Wikitology. In our previous work, we have carried out intensive experiments by extracting knowledge from Wikitology and evaluating the experiment on Support Vector Machine with 10-fold cross-validations. The results clearly indicate Wikitology is far better than other knowledge bases. In this paper we are comparing Support Vector Machine (SVM) and Na{\"{i}}ve Bayes (NB) classifiers under text enrichment through Wikitology. We validated results with 10-fold cross validation and shown that NB gives an improvement of +28.78{\%}, on the other hand SVM gives an improvement of +6.36{\%} when compared with baseline results. Na{\"{i}}ve Bayes classifier is better choice when external enriching is used through any external knowledge base.},
	author = {Hassan, Sundus and Rafi, Muhammad and {Shahid Shaikh}, Muhammad},
	file = {:C$\backslash$:/Users/DM/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassan, Rafi, Shahid Shaikh - Unknown - Comparing SVM and Na{\"{i}}ve Bayes Classifiers for Text Categorization with Wikitology as knowledge e.pdf:pdf},
	keywords = {20 News Group Knowledge base,Machine Learning,Na{\"{i}}ve Bay,Support Vector Machine,Text Categorization,Wikitology},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Comparing SVM and Na{\"{i}}ve Bayes Classifiers for Text Categorization with Wikitology as knowledge enrichment}},
	url = {https://arxiv.org/ftp/arxiv/papers/1202/1202.4063.pdf}
}

@misc{Mitchell,
	abstract = {This data set consists of 20000 messages taken from 20 newsgroups.},
	author = {Mitchell, Tom},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Twenty Newsgroups Data Set}},
	url = {https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups},
	urldate = {2018-11-07}
}

@INPROCEEDINGS{Shahi2018, 
	author={T. B. Shahi and A. K. Pant}, 
	booktitle={2018 International Conference on Communication information and Computing Technology (ICCICT)}, 
	title={Nepali news classification using Naïve Bayes, Support Vector Machines and Neural Networks}, 
	year={2018}, 
	volume={}, 
	number={}, 
	pages={1-5}, 
	keywords={Bayes methods;information resources;learning (artificial intelligence);multilayer perceptrons;natural language processing;pattern classification;support vector machines;text analysis;Naïve Bayes;support vector machines;automated news classification;predefined category;training news dataset;widely used machine learning techniques;Naive Bayes;automatic Nepali news classification problem;experiment the system;Nepali News Corpus;national news portals;TF-IDF based features;preprocessed documents;classification accuracy;linear SVM;Multilayer Perceptron Neural Networks;RBF kernel;Support vector machines;Neural networks;Training;Kernel;Feature extraction;Portals;Computers;Natural Language Processing;Machine Learning;Nepali news classification;TF-IDF;Naive Bayes;Support Vector Machine;Neural Networks}, 
	doi={10.1109/ICCICT.2018.8325883}, 
	ISSN={}, 
	month={Feb},}

@article{Shahi2018,
	author = {Shahi, Tej Bahadur and Pant, Ashok Kumar},
	doi = {10.1109/ICCICT.2018.8325883},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahi, Pant - 2018 - Nepali news classification using Na{\"{i}}ve Bayes, Support Vector Machines and Neural Networks.pdf:pdf},
	isbn = {9781538620519},
	journal = {Proceedings - 2018 International Conference on Communication, Information and Computing Technology, ICCICT 2018},
	keywords = {Machine Learning,Naive Bayes,Natural Language Processing,Nepali news classification,Neural Networks,Support Vector Machine,TF-IDF},
	mendeley-groups = {AIHI Thesis/Document Classification},
	pages = {1--5},
	title = {{Nepali news classification using Na{\"{i}}ve Bayes, Support Vector Machines and Neural Networks}},
	volume = {2018-Janua},
	year = {2018}
}

@techreport{Jadav2016,
	abstract = {Social media is a popular network through which user can share their reviews about various topics, news, products etc. People use internet to access or update reviews so it is necessary to express opinion. Sentiment analysis is to classify these reviews based on its opinion as either positive or negative category. First we have preprocessed the dataset to convert unstructured reviews into structured form. Then we have used lexicon based approach to convert structured review into numerical score value. In lexicon based approach we have preprocessed dataset using feature selection and semantic analysis. Stop word removal, stemming, POS tagging and calculating sentiment score with help of SentiWordNet dictionary have been done in preprocessing part. Then we have applied classification algorithm to classify opinion as either positive or negative. Support vector machine algorithm is used to classify reviews where RBF kernel SVM is modified by its hyper parameters which are soft margin constant C , Gamma $\gamma$. So optimized SVM gives good result than SVM and na{\"{i}}ve bayes. At last we have compared performance of all classifier with respect to accuracy.},
	author = {Jadav, Bhumika M and Scholar, M E},
	booktitle = {International Journal of Computer Applications},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jadav, Scholar - 2016 - Sentiment Analysis using Support Vector Machine based on Feature Selection and Semantic Analysis.pdf:pdf},
	keywords = {Na{\"{i}}ve Bayes,RBF kernel SVM,SVM,SentiWordNet,Sentiment analysis,Text mining},
	mendeley-groups = {AIHI Thesis/Document Classification},
	number = {13},
	pages = {975--8887},
	title = {{Sentiment Analysis using Support Vector Machine based on Feature Selection and Semantic Analysis}},
	url = {https://pdfs.semanticscholar.org/7746/93175a159160697b5748561446313186b846.pdf},
	volume = {146},
	year = {2016}
}

@techreport{Radford2018,
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9{\%} on commonsense reasoning (Stories Cloze Test), 5.7{\%} on question answering (RACE), and 1.5{\%} on textual entailment (MultiNLI).},
	author = {Openai, Alec Radford and Openai, Karthik Narasimhan and Openai, Tim Salimans and Openai, Ilya Sutskever},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training.pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Improving Language Understanding by Generative Pre-Training}},
	url = {https://gluebenchmark.com/leaderboard}
	year = {2018}
}

@techreport{Glorot2011,
	abstract = {The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsuper-vised fashion. Sentiment classifiers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bordes, Bengio - 2011 - Domain Adaptation for Large-Scale Sentiment Classification A Deep Learning Approach(2).pdf:pdf},
	mendeley-groups = {AIHI Thesis/Document Classification},
	title = {{Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach}},
	url = {http://www.cs.jhu.edu/},
	year = {2011}
}

@article{qiu2017limited,
	title={Limited individual attention and online virality of low-quality information},
	author={Qiu, Xiaoyan and Oliveira, Diego FM and Shirazi, Alireza Sahami and Flammini, Alessandro and Menczer, Filippo},
	journal={Nature Human Behaviour},
	volume={1},
	number={7},
	pages={0132},
	year={2017},
	publisher={Nature Publishing Group}
}

@article{lazer2018science,
	title={The science of fake news},
	author={Lazer, David MJ and Baum, Matthew A and Benkler, Yochai and Berinsky, Adam J and Greenhill, Kelly M and Menczer, Filippo and Metzger, Miriam J and Nyhan, Brendan and Pennycook, Gordon and Rothschild, David and others},
	journal={Science},
	volume={359},
	number={6380},
	pages={1094--1096},
	year={2018},
	publisher={American Association for the Advancement of Science}
}








