\documentclass[a4paper,twoside,phd]{BYUPhys}
% The BYUPhys class is for producing theses and dissertations
% in the BYU Department of Physics and Astronomy.  You can supply
% the following optional arguments in the square brackets to
% specify the thesis type:
%
%   senior  : Produces the senior thesis preliminary pages (default)
%   honors  : Produces the honors thesis preliminary pages
%   masters : Produces the masters thesis preliminary pages
%   phd     : Produces the PhD dissertation preliminary pages
%
% The default format is appropriate for printing, with blank pages
% inserted after the preliminary pages in twoside mode so you can
% send it directly to a two-sided printer. However, for ETD
% submission the blank pages need to be removed from the final output.
% The following option does this for you:
%
%   etd     : Produces a copy with no blank pages in the preliminary section.
%             Remove this option to produce a version with blank pages inserted
%             for easy double sided printing.
%
% The rest of the class options are the same as the regular book class.
% A few to remember:
%
%   oneside : Produces single sided print layout (recommended for theses less than 50 pages)
%   twoside : Produces double sided print layout (the default if you remove oneside)
%
% The BYUPhys class provides the following macros:
%
%   \makepreliminarypages : Makes the preliminary pages
%   \clearemptydoublepage : same as \cleardoublepage but doesn't put page numbers
%                           on blank intervening pages
%   \singlespace          : switch to single spaced lines
%   \doublespace          : switch to double spaced lines
%
% --------------------------- Load Packages ---------------------------------

% The graphicx package allows the inclusion of figures.  Plain LaTeX and
% pdfLaTeX handle graphics differently. The following code checks which one
% you are compiling with, and switches the graphicx package options accordingly.
\usepackage{ifpdf}
\ifpdf
  \usepackage[pdftex]{graphicx}
\else
  \usepackage[dvips]{graphicx}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Edited : Beeshanga
%
% If you need to include any code in the text use this package
% \usepackage{listings}
% It can be used to make key words bold, add colours, etc. Refer
% to http://en.wikibooks.org/wiki/LaTeX/Packages/Listings for
% more information.
%
% For theorems, propositions, proofs and assumtions use this
% package
% \usepackage{amsthm}
% For more information refer to the following website
% http://en.wikibooks.org/wiki/LaTeX/Theorems
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The fancyhdr package allows you to easily customize the page header.
% The settings below produce a nice, well separated header.
\usepackage{fancyhdr}
  \fancyhead{}
  \fancyhead[LO]{\slshape \rightmark}
  \fancyhead[RO,LE]{\textbf{\thepage}}
  \fancyhead[RE]{\slshape \leftmark}
  \fancyfoot{}
  \pagestyle{fancy}
  \renewcommand{\chaptermark}[1]{\markboth{\chaptername \ \thechapter. #1}{}}
  \renewcommand{\sectionmark}[1]{\markright{\thesection \ #1}}


% The cite package cleans up the way citations are handled.  For example, it
% changes the citation [1,2,3,6,7,8,9,10,11] into [1-3,6-11].  If your advisor
% wants superscript citations, use the overcite package instead of the cite package.
\usepackage{cite}
\usepackage{float}


% The makeidx package makes your index for you.  To make an index entry,
% go to the place in the book that should be referenced and type
%  \index{key}
% An index entry labeled "key" (or whatever you type) will then
% be included and point to the correct page.
%\usepackage{makeidx}
%\makeindex

% The url package allows for the nice typesetting of URLs.  Since URLs are often
% long with no spaces, they mess up line wrapping.  The command \url{http://www.physics.byu.edu}
% allows LaTeX to break the url across lines at appropriate places: e.g. http://www.
% physics.byu.edu.  This is helpful if you reference web pages.
\usepackage{url}
\urlstyle{rm}

% If you have a lot of equations, you might be interested in the amstex package.
% It defines a number of environments and macros that are helpful for mathematics.
% We don't do much math in this example, so we haven't used amstex here.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{cite}
\usepackage{amsxtra}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{multirow} % This is package for multi-rows in tables added on 7th July 2009 by Arif
%\usepackage{setspace}

% The caption package allows us to change the formatting of figure captions.
% The commands here change to the suggested caption format: single spaced and a bold tag
\usepackage[labelfont=bf,labelsep=colon]{caption}%[2008/04/01]
 \DeclareCaptionFormat{suggested}{\singlespace#1#2#3\par\doublespace}
 \captionsetup{format=suggested}


\usepackage{array}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{enumerate}

% Defining the symbols

% The hyperref package provides automatic linking and bookmarking for the table
% of contents, index, equation references, and figure references.  It must be
% included for the BYU Physics class to make a properly functioning electronic
% thesis.  It should be the last package loaded if possible.
%
% To include a link in your pdf use \href{URL}{Text to be displayed}.  If your
% display text is the URL, you probably should use the \url{} command discussed
% above.
%
% To add a bookmark in the pdf you can use \pdfbookmark.  You can look up its usage
% in the hyperref package documentation
\usepackage[bookmarksnumbered,pdfpagelabels=true,plainpages=false,colorlinks=true,
            linkcolor=black,citecolor=red,urlcolor=blue]{hyperref}

% ------------------------- Fill in these fields for the preliminary pages ----------------------------
%
% For Senior and honors this is the year and month that you submit the thesis
% For Masters and PhD, this is your graduation date
  \Year{2018}
  \Month{November XX,}
  \Author{Dione Morales}

% If you have a long title, split it between two lines. The \TitleBottom field defines the second line
% A two line title should be an "inverted pyramid" with the top line longer than the bottom.
  \TitleTop{Detecting health misinformation online}
  \TitleBottom{using deep learning methods}
  %\TitleBottom{Line 2 of the Tile} % edited Beeshanga
 \DegreeTitle{Bachelor of Engineering
 \\ Computer Engineering Stream} % edited Beeshanga

% Your research advisor
 \Advisor{Supervisors: Associate Professor Adam Dunn and Dr Rex Di Bona}

% The department undergraduate research coordinator
%  \UgradCoord{A}

% The representative of the department who will approve your thesis (usually the chair)
%  \DepRep{B}

% Acknowledge those who helped and supported you

  \Acknowledgments{
  \vspace{-1.5cm}
    \noindent I would like to acknowledge the countless number of people that have helped me get to where I am today.\newline
    I would also like to thank my supervisors, Rex Di Bona and Adam Dunn, for giving me the freedom and support to be able to work on my current topic. I am deeply grateful for the help and opportunities they have given me.
  }


% The title of the department representative
%  \DepRepTitle{Chair}
  \Statement{
    \noindent I, Dione Morales, declare that this report, submitted as part of the requirement for the award of Bachelor of Engineering in the School of Engineering, Macquarie University, is entirely my own work unless otherwise referenced or acknowledged. This document has not been submitted for qualification or assessment an any academic institution.
    \vspace{0.5cm}

    \noindent     Student's Name: Dione Morales

    \vspace{0.25cm}

    \noindent Student's Signature: Dione Morales (electronic)

    \vspace{0.25cm}

    \noindent     Date: September 9, 2018
    }

% The text of your abstract
\Abstract{
\vspace{-1.5 cm}
With the popularity and ubiquity of social media platforms in today's society, the amount and rate at which information propagates online greatly outnumbers the resources available that can evaluate the quality and credibility of the information that gets shared. This is becoming an increasingly growing issue due to the clickbait model that is commonly adopted by social media platforms and the lack of rigour surrounding the publishing of content online, causing an increase in the number of articles that contain misinformed content. This project aims to investigate the performance of deep learning techniques in evaluating the credibility of information of health-related articles.

}



% Statement of Candidate



\fussy

\begin{document}

 % Start page counting in roman numerals
 \frontmatter

 % This command makes the formal preliminary pages.
 % You can comment it out during the drafting process if you want to save paper.

 \makepreliminarypages


%\clearemptydoublepage
\doublespace
%\include{Publications/publications}

% \clearemptydoublepage
%\include{Organization/organization}

 \clearemptydoublepage
\singlespace
 % Make the table of contents.
 \tableofcontents

\clearemptydoublepage
% Make the list of figures
\listoffigures

\clearemptydoublepage
% Make the list of tables
\listoftables

\clearemptydoublepage

% Start regular page counting at page 1
\mainmatter
%
\chapter{Introduction}
\label{chap:Introduction}

With the popularity and ubiquity of social media platforms in today's society, the amount and rate at which information propagates online greatly outnumbers the resources available that can evaluate the quality and credibility of the information that gets shared. This is becoming an increasingly growing issue due to the clickbait model that is commonly adopted by social media platforms and the lack of rigour surrounding the publishing of content online \cite{Sommariva2018}, causing an increase in the number of articles that contain misinformed content \cite{germanFN} \cite{Vosoughi}.\newline 

The lack of resources that attempt to minimise the spread of misinformation can be attributed to the expert-level knowledge required to determine the credibility of information since a variety of factors such as the actual information, information sources, conflicts of interest and writing style of the article must be evaluated to be able to determine its quality and credibility. This issue is further exacerbated in specific domains such as for health-related content, as the spread of misinformation can have a detrimental effect on people and their communities that don't have access to these kinds of limited resources. \newline

Thus, by developing a method that is capable of automatically assessing the quality and credibility of an article - various methods that intervene between an article and a user can then be developed to minimise the spread and effects of misinformation as the reliance on expert-level knowledge is no longer present.

\section{Project Overview}
\label{sec:ProjectOverview}
This section details the scope of the project and its associated outcomes outlining the various tasks that must be accomplished to successfully complete the project.

One of the key components required to minimise the propagation of misinformation online is to have the ability of automatically evaluating and quantifying the credibility of articles. However, traditional automated methods - such as machine learning-based techniques, still require the domain knowledge of experts to be able to develop the features required by the model. Thus, this project aims to investigate the performance of Deep Learning-based (DL) techniques in evaluating the credibility of information within domain-specific articles via the classification of set criteria that have deemed to be highly correlated with articles that have low credibility. Specifically, this project will focus on evaluating the credibility of online health articles related to vaccination due to the commonly misinformed and controversial views associated with its effects \cite{Burgess2006}. \newline

\section{Motivation}
\label{sec:Motivation}
The propagation of misinformed content can cause people within specific communities to adopt beliefs and practices that can have harmful effects to themselves and also to the people around them. In the context of health-related content specifically, these beliefs and practices can take form in the misuse or mistreatment of medicine or illnesses. A popular example where this issue has emerged is with the misinformed views by certain communities on the ramifications of vaccinating children. \newline

The effects of vaccination and their relation to causing autism within children is a popular example where the belief of inaccurate facts has had a detrimental effect to specific communities and their surrounding environments. The measles outbreak in Minnesota which occurred in April 2017 is a specific case where the misinformed views on the effects of vaccination caused people within a Somali-American community located in the United States to forego the vaccination of their children causing an outbreak of measles, a disease which was declared to be eliminated from the United States in the year 2000 \cite{Hall2017}. Within this specific instance, it was determined that the outbreak of measles was caused by a decline in the coverage of vaccination due to concerns of its effects in relation to the causation of autism. This can be seen in the decreasing trend of the percentage for 24 month-old children that received the measles-mumps-rubella vaccine which fell to a low of 40\% in 2014 where it was at a 90\% high 10 years prior.

\section{Aims}
\label{sec:Aims}

With the primary objective of this project being the evaluation on the effectiveness of deep learning models in determining the credibility of online vaccine-related health articles. Due to the complexity of this project, a set of activities - divided into main goals and stretch goals, have been defined to ensure that the completion of this project remains feasible in the given time frame. The completion of all activities categorized as main goals will signal the realization of the primary objective and the completion of the project. Stretch goals are activities of interest that have been identified as non-essential to the completion of the primary objective but will be worked on after the completion of the project.

\subsubsection{Main Goals}
\label{sec:MainGoals}
\begin{itemize}
	\item Implement and evaluate the performance of machine learning methods for assessing the quality and credibility of vaccine-related text.
	\item Implement a deep neural network method to assess the quality and credibility of vaccine-related text and compare the performance with the previous approaches.
\end{itemize}

\subsubsection{Stretch Goals}
\label{sec:StretchGoals}
\begin{itemize}
	\item Evaluate the effect of transfer learning methods on the training time and performance of the proposed deep neural network method.
\end{itemize}


\chapter{Background and Related Work}
\label{chap:LitReview}

A literature review has been conducted to develop an understanding on the research that has been done in the assessment of the credibility of information, specifically in the context of information related to health and the limitations and capabilities of machine learning techniques and how it differs from deep learning-based methods for the task of document classification. 

\section{Assessing the Quality and Credibility of Health Information}
\label{sec:AssessingInformation}

To have the capability of automating the process of evaluating the quality or credibility of online information, a definition that outlines of what is required by an article to be considered as a credible source of information must be developed. While there has been a significant amount of research that has been done on the development of tools and frameworks that aim to assess the credibility of online health information, there is currently no standardized method or benchmark that is universally used. Tools and frameworks that have been identified to be applicable in the context of this project are: DISCERN \cite{DISCERN}, HealthNewsReview \cite{HealthNewsReview} and Quality Index for health-related Media Reports (QIMR) \cite{QIMR}.

\subsection{Checklists and Criteria}
\label{sec:Checklists}

\subsubsection{DISCERN}
\label{sec:DISCERN}

DISCERN \cite{DISCERN} is a questionnaire designed to assess the reliability of a publication, it consists of 16 questions each with a Likert scale, ranging from 1 (no) to 5 (yes) and is divided into 3 sections. The first section (questions 1 - 8) investigate the reliability of the information and is comprised of questions such as "Are the aims clear?", "Is it balanced and unbiased?" and "Does it refer to areas of uncertainty?". The second section (questions 9 - 15) assesses the quality of information provided by the publication for treatment choices and is composed of questions such as "Does it describe the risks/benefits of each treatment?" and "Does it provide support for shared decision-making?". The final section (question 16) assesses the overall rating of the publication ("Based on the answers to all of the above questions, rate the overall quality of the publication as a source of information about treatment choices").


\subsubsection{HealthNewsReview}
\label{sec:HealthNewsReview}

HealthNewsReview \cite{HealthNewsReview} provides a set of 10 criteria designed to act as a framework for evaluating the credibility of health-related media. The criteria is based on the various elements that all health-related media should consist of, the criteria is composed of criterion such as "Does the story compare the new approach with existing alternatives?", "Does the story use independent sources and identify conflicts of interest?" and "Does the story appear to rely solely or largely on a news release?".

\subsubsection{QIMR}
\label{sec:QIMR}

QIMR \cite{QIMR} is a tool developed to monitor the quality of health research reports presented in the media. The tool considers 5 main factors that have been deemed to be correlated with the quality of research reports based on interviews with health journalists and researchers. The 5 main factors are: background information provided, sources of information used, manner in which results were analysed, context of the research and the validity of their methodology.

\subsection{Studies Applying Checklists to Health Information Online}
\label{sec:ChecklistStudies}

There has been an extensive amount of research conducted which aimed to assess the quality of content for various health-related domains using one of the aforementioned tools \cite{CanteyBanasiak2017} \cite{Cipriani} \cite{Kaicker2010} \cite{Som2012}. A factor shared by these commonly performed studies however, is the use of experts to leverage the tool in assessing the quality and credibility of the information, indicating that commonly used tools such as DISCERN is designed to be used by domain experts in order to produce reliable and consistent results. This sentiment is shared by Batchelor et al. \cite{Batchelor2009} who evaluated performance of the DISCERN tool when used by health professionals and patients. Due to the resource intensive nature of expertly performing this task, the number of articles evaluated, among the various studies examined have been limited ranging between 10 - 300 information sources. This highlights the issue that these tools are not designed to be capable of evaluating the quality and credibility of articles at the scale and speed required to match the pace of online activity and thus must be adapted for the automation of this process. While there has been work in adapting criteria to better encompass the factors that are correlated to the quality and credibility of information within a specific context, such as by Matsoukas et al. \cite{Matsoukas2008}, who expanded the DISCERN tool to improve its capability in assessing the quality of online information, there has not been any published work found that aims to adapt these tools to provide the capability of autonomously assessing the quality and credibility of resources.

\section{Document Classification Methods}
\label{sec:DocumentClassification}

The task of document classification is a heavily researched topic due to its wide number of applications in various domains. Formally, document classification is defined to be the task of finding a classifier $f: D \rightarrow L$ where $D = \{d_1, d_2, \dots, d_n\} $ is a collection of documents and $L = \{l_1, l_2,\dots, l_k\}$ is the set of possible labels that a document $d$ can be classified as. 
\begin{equation}
\label{eq:TextClassification}
f: D \rightarrow L \text{ where } f(d) = l
\end{equation}
Common applications of document classification algorithms are: organization and filtering of news articles, document retrieval, opinion mining, email classification and spam filtering \cite{Aggarwal2012}.

Traditional machine learning-based approaches for document classification, such as Naive Bayes, Support Vector Machines (SVM) and Random Forests, require the manual extraction of features \cite{Aggarwal2012} \cite{Allahyari2017} \cite{Korde2012} \cite{Pasupa2016} as they are incapable of performing feature learning \cite{Basheer2000}. These features are typically either hand-crafted and domain specific, requiring expert-level knowledge for the task domain \cite{Pasupa2016} or are simple and general but prone to the loss of information (e.g. being unable to account for context) since these features, such as term frequency-inverse document frequency (TF-IDF) scores or bag of words (BoW) models, are unable to represent and account for the sequential nature of text.

\subsection{Representing Text as Features}
\label{sec:TextRepresentations}

Due to the unstructured nature of natural text, it is common practice for natural language processing (NLP) tasks to transform the text into a structured representation that minimises the loss of information stored within the original text whilst allowing the model to learn and analyse the features associated with the classification task. However, despite traditional representation methods such as count-based approaches (e.g. TF-IDF) or probabilistic-based approaches (e.g. N-Grams) showing respectable performances in the classification of texts \cite{Zhang} \cite{Zhou2015}, these representations are typically sparsely distributed and have a high dimensionality that scales with the vocabulary size, has difficulty in generalising over unfamiliar information or writing styles \cite{Rosenfeld2000} as the texts are represented using simple mechanisms that are unable to represent high-level information such as context or the semantic relationship of words. Due to the high dimensionality and sparseness of these features, they are computationally intensive to calculate and can require a relatively larger amount of space for storage.\newline

Word embeddings are a more sophisticated language model that overcome these limitations as they are created by learning word vectors through the optimisation for a different task e.g. the prediction of a word based on its context. Through this, the learned embeddings are able to store information such as the similarity of different contexts, syntactic and semantic information within the text and analogies. These embeddings are relatively more efficient and require less time to compute due to their smaller dimensionality. Whilst these embeddings are able to be created using shallow neural networks, deep learning-based models for NLP tasks utilise these embeddings which can also be at the character, phrase or sentence levels \cite{Young}. Word embeddings, such as Word2Vec or GloVe, have produced state-of-the-art results in not only classification tasks \cite{Howard2018} but also for a wide range of NLP tasks such as image annotation \cite{Weston} and sentiment analysis \cite{Cambria2017}, which used the popular word2vec embedding proposed by Mikolov et al. \cite{Mikolov}


\subsection{Machine Learning Methods for Document Classification}
\label{MLDocumentClassificaiton}

\subsubsection{Naive Bayes Classifier}
\label{sec:NaiveBayes}

Naive Bayes is a probabilistic classifier that functions on an assumption in how the data (i.e. the words in a document) is generated. The assumption that these Bayesian classifiers are based on is that the distribution of different words within a corpus are independent from each other. Despite this assumption clearly being wrong when considering the distribution of words within a document (due to the sequential nature of text), Naive Bayes classifiers are still able to perform well in document classification tasks such as the filtering of spam emails \cite{Sahami}.

Naive Bayes classifiers utilize Bayes' theorem, which attempts to find the probability of an event $B$ occurring given some prior condition $A$ i.e. $P(B| A)$. In the context of document classification, Naive Bayes classifiers classifies a document $d$ by calculating the probabilities that the document belongs for all labels $l_i \in L$ and then selecting the highest probability \cite{Allahyari2017}:
\begin{equation}
P(L = l_i| d) = \frac{P(l_i)P(d| l_i)}{P(d)}
\end{equation}

According to Aggarwal et al. \cite{Aggarwal2012}, there are two main types of Naive Bayes classifiers that are commonly used, they are the multivariate Bernoulli model and the multinomial model. The main discriminating factor between these two models is that the Bernoulli model does not take into account the frequency of words as it represents a document using a vector of binary features which signify the presence or absence of words, based on some vocabulary, for a given document. This is in contrast with the multinomial model, which accounts for the frequency of words as the document is represented using a BoW model. Deciding on which model to use for document classification largely depends on the size of the vocabulary as, according to McCallum et al. \cite{McCallum1998}, the multinomial models almost always outperforms the Bernoulli models if the size of the vocabulary is large ($> 1000$) or even if the size has been optimally chosen for both models.

\subsubsection{Support Vector Machines}
\label{sec:SVM}

SVMs are a type of supervised machine learning-based binary classifiers that are extensively used for document classification due to their capability of handling the high dimensional and sparse nature of the common techniques used to represent text documents \cite{Informatik1997}. SVMs are able to classify a document by using a hyperplane to separate the different classes into separate regions. The hyperplane used is determined by choosing the one that maximises the margin of separation (i.e. The Euclidean distance between the hyperplane and all data points in the representation space) between the two classes \cite{Aggarwal2012}.

Consider the illustration shown in Figure \ref{fig:SVMExample} that presents three  two-dimensional hyperplanes, $A$, $B$ and $C$, which separates the classes 'x' and 'o'. Visually, we can determine that $A$ is the hyperplane that maximises the margin of separation, thus, $A$ will be used as the decision rule to classify any new article based on its location in the representation space with respect to $A$. Mathematically, determining which hyperplane to use as the decision rule is an optimisation problem that attempts to maximise the margin represented as shown in Equation \ref{eq:MaxSVM} but is often re-framed to the minimisation of Equation \ref{eq:MinSVM}:

\begin{equation}
\text{maximise: } \frac{1}{2}||\frac{1}{w}||^2
\label{eq:MaxSVM}
\end{equation}

\begin{equation}
\text{minimise: } \frac{1}{2}||w||^2
\label{eq:MinSVM}
\end{equation}


\begin{figure}[H]
	\centering
	\includegraphics[totalheight=7cm]{images/svm-example.png}
	\caption{Determining the optimal hyperplane \cite{Aggarwal2012}}
	\label{fig:SVMExample}
\end{figure}

This technique of determining the hyperplane requires that the two classes are linearly separable. In situations where this isn't true, the kernel trick \cite{aizerman67theoretical} is applied which is essentially a function that maps data in a particular representation space to another representation space of a different dimension. This change in dimensionality can allow the classes to become linearly separable and thus a hyperplane can then be constructed that separates each class as illustrated in Figure \ref{fig:KernelTrick}.

\begin{figure}[H]
	\centering
	\includegraphics[totalheight=7cm]{images/kernel-trick.png}
	\caption{(Left) Non-linearly distributed data of two classes in a 2D representation space. \newline
		(Right) Linearly separable data of the same classes in a 3D representation space after the application of a kernel function \cite{UnderstandingKernel}.}
	\label{fig:KernelTrick}
\end{figure}

\subsection{Deep Learning Methods for Document Classification}
\label{sec:DLDocumentClassification}

Deep learning models are a class of machine learning models that have the capability of automatically learning a hierarchical representation of data \cite{Basheer2000}. These hierarchical representations are constructed through the use of artificial neural networks, the main underlying mechanism of deep learning models. The most commonly used models for document classification are Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs) - a variant of RNNs and Convolutional Neural Networks (CNNs). \newline

For the task of text classification, there are commonly used, open datasets for specific classification tasks that aim to provide a benchmark in which the performance of different implementations can be compared and to facilitate the data needs of deep learning models. 

\subsubsection{Recurrent Neural Networks}
\label{sec:RNN}

RNNs and LSTMs are commonly used for document classification, or NLP tasks in general, due to their ability to create language models that are able to capture the context and relationships of words within documents over long distances and represent this information at a much more sophisticated level when compared to traditional language models such as TF-IDF or n-grams \cite{Young}. \newline
 
RNNs have had widespread use in solving NLP-related tasks due to their ability in capturing the sequential nature of text at the character, word or sentence level. Consequently, RNNs are capable of creating language models that account for the semantic meaning of words based on the previously occurring words in the sentence, allowing models to be capable of understanding the difference between similar words or phrases (e.g. that the word "dog" is likely referring to the animal whereas "hot dog" would be more likely to refer to food). Accounting for RNNs' capability in handling variable length inputs (e.g. long sentences, paragraphs or documents), they have shown to produce state-of-the-art results in classification tasks such as sentiment, question and topic classification \cite{Howard2018}.\newline

The simplest form of an RNN, as illustrated in Figure \ref{fig:RNN}, consists of three layers: the input layer $x_t$, hidden state $s_t$ and the output layer $o_t$, where $t$ represents the current timestep. The input layer is typically represented as a one-hot encoding or embedding, the output layer is the resulting output which can take many forms, most commonly, it is the output of the softmax function and the hidden state is essentially the memory of the network as it captures and incorporates the information from previous timesteps into the current one. The hidden state is calculated by evaluating Equation \ref{eq:HiddenState}:

\begin{equation}
s_t = f(Ux_t + Ws_{t-1})
\label{eq:HiddenState}
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[totalheight=6cm]{images/RNN.jpg}
	\caption{Layout of a simple RNN \cite{RNNDiagram}}
	\label{fig:RNN}
\end{figure}

Where $f$ is a nonlinear function e.g. Rectifier Linear Unit (ReLU), $U$, $V$ and $W$ are weight matrices that are shared across timesteps \cite{Young}. \newline

LSTMs \cite{Hochreiter1997} are a variant of RNNs that improves on it by introducing a 'forget' gate which regulates a cell's state within the network as it can allow information from different cells to be removed or added. This addition allows LSTMs to overcome the vanishing and exploding gradient problem \cite{Socher} that feedforward networks i.e. RNNs are prone to.

\subsubsection{Convolutional Neural Networks}
\label{sec:CNN}

Despite CNNs being initially developed for object recognition tasks \cite{LeCun1999}, they are commonly employed for text classification tasks such as sentence classification and sentiment analysis as they have shown to produce competitive results when compared to RNNs and LSTMs \cite{Collobert2011} \cite{Kim} \cite{Nogueira}. \newline

In the context of text classification, a typical CNN is composed of the following main elements \cite{Young}:

\begin{itemize}
	\item Kernels - Convolutional filters that acts as a sliding window function through an embedding matrix. CNNs typically employ hundreds of kernels each of which learns to extract for specific n-gram pattern.
	\item Pooling layer - Commonly either a max or average pooling layer, which maps the input to a fixed dimension in order to reduce the dimensionality of the output and ensuring that the most salient n-gram features of a sentence is kept.
	\item Nonlinear activation function - Such as a ReLU function applied to the results to output a prediction.
\end{itemize}

By stacking the kernels and pooling layers, deep CNNs can be constructed which can automatically capture an abstract and rich representation of the information \cite{Young}. These representations are considered to be efficient when compared to traditional representation methods (e.g. n-grams) as they don't require the storage of the entire vocabulary and is not as computationally intensive. CNNs are also more computationally efficient when compared to RNNs and LSTMs, CNNs typically require larger amounts of data in order to produce competitive results against its RNN and LSTM counterparts due to the higher number of trainable parameters that CNNs have. Another limitation of CNNs is their inability to model long-distance relationships and preserving the sequential nature of text within its representations.

\subsection{Classifying Documents with Limited Labelled Examples}
\label{sec:TransferLearning}
Typically, large amounts of training data is required to train a deep learning model in learning the language model for state of the art results, in the task of document classification for instance, the size of commonly used non-domain specific datasets range from hundreds of thousands of training examples to millions \cite{Conneau2017} \cite{Zhang}. For situations where you are required to procure a completely new dataset, such as training a deep learning model for a non-standard text classification task, it can be unfeasible  or not worthwhile to invest the time and resources in creating the dataset. 

\subsubsection{Transfer Learning}
\label{sec:TransferLearningReview}

Transfer learning involves the repurposing of an already existing deep learning model that has been trained to perform a different, but similar, task using an already existing and available dataset. It functions on the idea that the features automatically learned by a model for some similar task is general enough such that they can then be utilised on the completely new task. 

Howard et al. \cite{Howard2018} has proposed a transfer learning methodology for text classification which was designed to be able to develop models with state-of-the-art results for tasks where there is a limited amount of training data available. \newline

The proposed method involves the use of an inductive learning technique \cite{Pan2009}:
\begin{enumerate}
	\item Create a language model to capture general features of the language by training on a general-domain corpus (e.g. wikitext103 \cite{Merity2016}).
	\item Learn task specific features by fine-tuning the language model using the target task data.
	\item Adapt the high-level representations of the classifier that uses the language model, while preserving the low-level representations, using gradual unfreezing \cite{Howard2018}
\end{enumerate}

Whilst transfer learning is useful to train models for non-standard tasks where access to data is limited, it is also an optimisation technique, using previously acquired knowledge, to save time or improve model performance \cite{Pan2009}. This potential performance improvement is illustrated in Figure \ref{fig:TransferLearning} and is also demonstrated by Howard et al. \cite{Howard2018} for common text classification tasks such as question, topic  classification and sentiment analysis.

\begin{figure}[H]
	\centering
	\includegraphics[totalheight=7cm]{images/transfer-learning.png}
	\caption{Potential improvements in performance using transfer learning \cite{Browniee2017}}
	\label{fig:TransferLearning}
\end{figure}

\section{Discussion}
\label{sec:LitReviewDiscussion}

\subsection{Scaling the Assessment of Quality and Credibility of Health Information}
\label{sec:ScalingAssessment}

Among the studies that aim to assess the credibility and quality of information, the limited amount of articles that were evaluated was a common theme \cite{Batchelor2009} \cite{CanteyBanasiak2017} \cite{Cipriani} \cite{Kaicker2010} \cite{Som2012}. This makes evaluating the credibility of online articles impractical when using the existing assessment tools as described in Section \ref{sec:Checklists}, due to the persistence, pace and volume of online content. Due to the capabilities of deep learning models, they can be leveraged to accomplish the task of automatically assessing the credibility of online articles as they have shown the ability to develop high-level representations of the language and be able to differentiate between credible and misinformed articles.


\chapter{Methods}
\label{chap:Methods}

\section{Approach}
\label{sec:Approach}

To achieve the overarching goal of this project which, as discussed in Section \ref{sec:Aims}, is to evaluate the performance of automation techniques, specifically, using deep learning models to automate the resource intensive task of determining the credibility of online articles, the task must be framed in a way that can be used by the models in order to be evaluated. \newline

This involved obtaining training and testing data consisting of online vaccine-related articles along with its associated credibility that were manually determined by a team of health professionals using a pre-existing framework. Details on the procurement process and the framework used to evaluate an article's credibility is outlined in Section \ref{sec:StudyData}. \newline

Once the dataset was created, a set of baseline models were implemented in order to have a baseline reference for the performance of the deep learning model. The baseline reference models comprised of the performance of widely used text classifiers, Naive Bayes (NB) and Support Vector Machines (SVMs), in combination with a variety of textual representation methods and the state-of-the-art text classifier FastText. The implementation details of the baseline models and the implemented deep learning model is further discussed under Section \ref{sec:SystemModel}. \newline

Section \ref{sec:Experiments} then outlines the comparison and analysis methodology for the performance of the deep learning model against the baseline implementations based on its ability to correctly predict the label for each criteria and other factors with regards to its feasibility in deploying the model for a real-world application.

\section{Study Data}
\label{sec:StudyData}

\subsection{Dataset}
\label{sec:Dataset}

For the purposes of training and evaluating the performance of the baseline and deep learning models for this non-standard classification task, a dataset consisting of 3.5K unlabelled and manually labelled articles vaccine-related articles have been procured. \textit{\textbf{Insert that stuff Adam talked about in email feedback}}. Due to the infeasibility of developing the expert-level knowledge and skill set required to manually label the articles given the time constraints of this project, the articles within the dataset were manually labelled by the team that developed the credibility criteria which will be discussed in Section \ref{sec:CredibilityCriteria}.\newline

The articles used during the labelling process were collected by selectively accessing the extracted URLs embedded within tweets that contained specific keywords related to vaccination on Twitter. This was done to ensure that the articles used represented, to some degree, the articles that are most likely to contribute to the effects of the propagation of misinformation as the articles that have been collected and labelled are the ones being shared and discussed online. Once labelled, the article's credibility is then quantified using a credibility score which is equivalent to the total number of criteria satisfied by the article.

\subsection{Credibility Criteria}
\label{sec:CredibilityCriteria}
The seven criteria defined in Table \ref{table:Criteria} was developed by a team of three health professionals for a separate and unpublished research project. This set of criteria will be used to define the structure and content that a vaccine-related health article must have in order to be considered as a credible source of information. The development and selection of the criteria was largely inspired by the tools and checklists discussed in Section \ref{sec:AssessingInformation} which has been adapted specifically for the evaluation of online articles pertaining to vaccines. Prior to creating the dataset for this project, a small pilot experiment was conducted by the team that developed the criteria to ensure that the simplicity and objectivity of each criteria allowed for consistent and reliable labels to be produced. The pilot experiment involved each team member manually labelling the same set of \textbf{\textit{200 hundred billion}} articles after which each member's labels were compared and adjustments to the criteria were applied if a particular criteria was susceptible to inconsistent labelling.



\subsection{Study Data Limitations}
\label{sec:StudyDataLimitations}
Because of the selective collection process for the articles, the procured dataset has become susceptible to sampling bias as it placed a higher priority on collecting articles that were discussed and shared rather than creating a collection of articles that showed a representative distribution of the credibility of vaccine-related articles published and available online. Effects of the biased sampling methods is illustrated in Figure \ref{fig:LabelDistribution} which shows that, for the majority of the categories, the articles collected are highly skewed to either satisfy or fail to satisfy a specific criteria. However, other factors such as the varying difficulty to satisfy a specific criteria also affect the imbalance of the label distribution.


\begin{figure}[H]
	\centering
	\includegraphics[totalheight=6cm]{images/label-distribution.png}
	\caption{Distribution of labels amongst the collected articles.}
	\label{fig:LabelDistribution}
\end{figure}

This also highlights an issue in regards to the quantification of an article's credibility. Since the credibility score of an article is defined to be the total number of criteria that the article satisfies which implies that the weighting, or contribution, of each criteria are equivalent in making the article a more credible source. However, this is not reflective of the credibility criteria as the difficulty in satisfying a specific criteria, or its contribution in making an article more credible varies between each one. This causes the credibility score of an article to be biased towards credibility scores that have the highest number of combinations as shown in Figure \ref{fig:ArticleDistribution} making it difficult to differentiate and rank articles that achieved similar scores by satisfying different combinations of the credibility criteria.

\begin{figure}[!htb]
	\centering
	\includegraphics[totalheight=6.5cm]{images/article-score-distribution.png}
	\caption{Distribution of article scores amongst the manually labelled articles.}
	\label{fig:ArticleDistribution}
\end{figure}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{0.3cm}|p{5cm}|p{10cm}|}
		\hline
		\# & Criteria                                                                           & Description                                                                                                                                                                                                                                                                                                                                                             \\
		\hline
		1  & Identifies the information sources supporting a position            & Score 1 if the article clearly indicates what sources of information are used to support its main claims or statements. \\
		\hline
		
		2  & Uses information sources based on objective, scientific research. & Score 1 if the article's main claims are based on objective, scientific research.\newline Score 0 if the article uses anecdotal evidence exclusively to support its main claims.                                                                                                                                                  \\
		\hline
		
		3  & Communicates the strength or weakness of the evidence used to support a position. & Score 1 if the article includes adequate details about the level of evidence offered by the research.                                                                                                                                                                                                                        \\
		\hline
		4  & Does not exaggerate or overstate a position.                         & Score 0 if the article: \newline- Uses unjustified sensational language \newline- Presents information in a sensational, emotive or alarmist way \newline- Selectively or incorrectly presents evidence                                                                                                                                       \\
		\hline
		5  & Presents information in a balanced manner. & Score 1 if the article:\newline- Identifies/acknowledges uncertainties and limitations in the research\newline - Acknowledges where an issue is controversial, and includes all reasonable sides in a fair way\newline - Uses a range of information sources\newline - Provides a balanced description of the strengths and weaknesses of the study
		\\
		\hline
		6  & Uses clear, non-technical language that is easy to understand. & Score 1 if the article:\newline - Is professionally written, with proper grammar, spelling, and composition\newline - Defines any technical jargon, or uses everyday examples to explain the technical terms or concepts                                                                  \\
		\hline
		7  & Is transparent about sponsorship and funding. & Score 1 if the article:\newline - Clearly distinguishes content intended to promote or sell a product from service from educational and scientific content\newline - Discloses sources of funding for the organisation/website
		\\
		\hline                                                                    
	\end{tabular}
	\caption{Credibility Criteria}
	\label{table:Criteria}
\end{table}



\section{Model Implementation}
\label{sec:SystemModel}

\subsection{Preprocessing}
\label{sec:ModelPreprocessing}

Preprocessing of the unlabelled and expert-labelled articles of the dataset to obtain its raw article text was performed by removing the superfluous content from the web-scraped pages such as the HTML, CSS and JavaScript elements. They were removed as they were deemed to have no contribution to the credibility of the article's content with regards to the credibility criteria. Once the unstructured article text was obtained, it was transformed into a structured representation via tokenising each article by sentences that consisted of unigrams. The entire process was ensured to be general and simple as the various features that have been implemented and tested would require various kinds of transformations as explained in further detail under their respective section in Section \ref{sec:ModelFeatureSelection}.

\subsection{Feature Selection}
\label{sec:ModelFeatureSelection}



\subsubsection{Bag of Words}
\label{sec:FeatureSelectionBoW}
Two n-gram BoW variants, a unigram and bigram BoW model, was constructed using the expert-labelled articles both of which were limited to a maximum of 20K n-grams. Similar BoW models were also constructed for different n-gram amounts, specifically, BoW models for 5K and 10K n-grams were also tested. Common English stopwords such as 'the', 'is' and 'are' listed in NLTK's English stopwords corpus \cite{Bird2009} were removed from the unigram BoW model but remained for the bigram BoW model.

\subsubsection{Term Frequency - Inverse Document Frequency}
\label{sec:FeatureSelectionTFIDF}
TF-IDF scores of the unigrams extracted from the expert-labelled articles which were limited to the top 10K scores were calculated and used for evaluation. The top 10K TF-IDF scores consisted of scores for unigrams that appeared in at least two expert-labelled articles with no maximum occurrence limit.

\subsubsection{GloVe}
\label{sec:FeatureSelectionGloVe}

A pre-trained GloVe vector \cite{pennington2014glove} was also used for the evaluation of the ML-based models. This was achieved by using the mean value of each word's vector representation. The GloVe vector used was trained on six billion tokens, with a vocabulary size of 400K and a has dimensionality of 300.

\subsubsection{Language Models}
\label{sec:FeatureSelectionLM}

A non-domain specific language model (LM) was constructed for evaluating the DL-based model. This was achieved by training a Quasi-Recurrent Neural Network (QRNN) using the same hyperparameter settings and training scheme outlined in \cite{bradbury2016quasi} on the Wikitext-103 corpus. Wikitext-103 is a corpus consisting of 28K Wikipedia articles that meet either the 'Good' or 'Featured' article criteria \cite{Merity2016}. \newline

The LM was then fine-tuned using the techniques proposed by Howard et al. \cite{Howard2018} by using the 3.5K unlabelled articles from the dataset in order to learn the task-specific features for classification. The Wikipedia articles used to create the non-domain specific LM also underwent the same preprocessing process described in Section \ref{sec:ModelPreprocessing} sans the removal of webpage elements. 



\subsection{Model Selection}
\label{sec:ModelSelection}

For the software used to implement the models, the machine learning-based models were implemented using scikit-learn, an open source machine learning library in Python \cite{scikit-learn}. The QRNN model for the language model and classifier was implemented using PyTorch \cite{paszke2017automatic} and the open-source software developed by Bradbury et al. \cite{bradbury2016quasi}. 

\subsubsection{Naive Bayes}
\label{sec:ModelNB}

A multinomial NB classifier was used for evaluation, based on the reasoning discussed in Section \ref{sec:NaiveBayes}. A total of seven classifiers, one for each credibility criteria, was constructed whose hyperparameters were optimised via grid search and evaluated using 10-fold cross-validation. The optimal hyperparameters for all classifier were identical with an alpha value of 1.0, with fit prior set to true and no class prior. 

\subsubsection{Support Vector Machine}
\label{sec:ModelSVM}

A linear support vector classifier (linear SVC), an SVM with a linear kernel, was used for evaluation. Similar to Naive Bayes, a classifier was trained for each criteria and the hyperparameter values were optimised and evaluated using grid search and 10-fold cross-validation respectively. Whilst the resulting grid search for each classifier returned varying results for the optimal hyperparameter values, the results reported in Section \ref{sec:BaselineResults} used the same hyperparameter values for all classifiers. This was due to the performance of each classifier having similar results and remaining within the standard deviation of each other.  

\subsubsection{Quasi-Recurrent Neural Network}
\label{sec:ModelQRNN}

In addition to using a QRNN for constructing the fine-tuned LM, another variant was implemented as a classifier. The architecture of the model was adapted from the classifier that Bradbury et al. \cite{bradbury2016quasi} used for sentiment classification. From this model, the number of units within each layer was increased to 2500 and the embedding size was changed to 150. The model was trained for up to 20 epochs with a batch size of 100, however the model began to converge after only 5 epochs. In contrast to the ML-based classifiers, only a single QRNN was used to predict the correct label for each criteria via a softmax applied to the output layer.

\section{Experiments}
\label{sec:Experiments}

Considering the context of this project, other factors such as the size of the model and its training speed must be considered in addition to its performance on correctly producing the correct labels to determine the feasibility of deploying the model for a real-world application. 

\subsection{Accuracy}
\label{sec:AccuracyExperiments}

Two experiments have been conducted to evaluate the performance of a model. The credibility classification experiment is designed to determine the model's ability to produce the correct label on a criteria-level allowing for a more detailed insight on the reasoning for a particular article's credibility score. Whereas the low credibility identification experiment is designed to determine the model's ability to correctly determine whether an article is considered to have low credibility or not for situations where the detailed insight is not required.

\subsubsection{Credibility Classification Experiment}
\label{sec:CredibilityClassificationExperiment}
The credibility classification experiment aims to determine a model's ability in correctly identifying whether an article either satisfies or doesn't satisfy a certain criteria. For each criteria, the respective NB and SVM model is employed along with the QRNN model and is then evaluated by measuring the micro averaged f1-scores. This experiment attempts to assess the feasibility in providing insight on the factors that attributed to the credibility of an article.


\subsubsection{Low Credibility Identification Experiment}
\label{sec:LowCredibilityIdentification}
The Low Credibility Identification Experiment was designed to evaluate the capability of a model to identify low credibility articles which have been defined to be articles with a credibility score $< 3$. The manually labelled articles are separated into two separate classes based on this condition and the best performing model, which was determined from the results of the credibility classification experiment, is then applied and evaluated using the resulting micro averaged f1-scores. 

\subsection{Training}
\label{sec:TrainingExperiments}
The training time for a model is another factor that must be considered when determining the feasibility of deploying and using a model on a real-world application. This is because the model will require to be constantly re-trained when additional article samples are incorporated as the model encounters new and never before seen information. To ensure consistent results, Amazon Web Service's EC2 instances \cite{AWS} have been used to act as an isolated environment for timing the training of the NB, SVM, QRNN classifiers and also  the creation of the LM. The reported training times in Section \ref{sec:TrainingTime} are for the best performing NB and SVM-based models, which were trained within a c5.2xlarge instance utilising all CPU cores. The reported training times for the QRNN classifier and LM were obtained using the P3 instances and utilising only a single K80 GPU.


\subsection{Storage}
\label{sec:StorageExperiments}
To minimise the frequency in which a model will have to be re-trained or re-created, storing the model for later use is required. Since the entire implementation was completed using Python, the Python module \textit{pickle} was used to store the models externally. Pickle serialises any Python object into a byte stream allowing a model to be persistently stored. The NB and SVM-based models were stored using pickle and the size of the resulting byte stream was then measured. The QRNN classifier and LM were stored using PyTorch's inbuilt save function, that acts as a wrapper over the \textit{pickle} module which provides storage optimisations for PyTorch objects. The resulting byte stream is then recorded and reported in Section \ref{sec:StorageRequirements}.



\chapter{Results}
\label{chap:Results}

\textit{To justify the decision to treat each criteria as a set of binary classification tasks rather than a single multi-label classification task, experiments that compare the performance in terms of accuracy, speed, storage and training time for binary and multi-label classifiers will be conducted.}


\section{Classification Performance}
\label{sec:ClassificationPerformance}
\subsection{Machine Learning Models}
\label{sec:BaselineResults}
\textbf{**REMEMBER TO RERUN EXPERIMENTS WHEN LABELLING IS COMPLETED**}
- 
\begin{table}[H]
	\centering
	\begin{tabular}{|p{1.7cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|}
		\hline
		Model & Criteria 1                                                                           & Criteria 2    & Criteria 3 & Criteria 4 & Criteria 5 & Criteria 6 & Criteria 7                                                                                                                                                                                                                                                                                                                                                          \\
		\hline
		NB + BoW  & 0.79 \newline ($\pm$ 0.16) & 0.70 \newline ($\pm$ 0.12) & 0.86 \newline ($\pm$ 0.07) & 0.84 \newline ($\pm$ 0.13) & 0.79 \newline ($\pm$ 0.04)  & 0.75 \newline ($\pm$ 0.07)  & 0.82 \newline ($\pm$ 0.07)   \\
		\hline
		
		SVM + BoW  & 0.82 \newline ($\pm$ 0.7) & 0.70 \newline ($\pm$ 0.02) & 0.86 \newline ($\pm$ 0.03) & 0.80 \newline ($\pm$ 0.03) & 0.68 \newline ($\pm$ 0.07)  & 0.75 \newline ($\pm$ 0.7)  & 0.86 \newline ($\pm$ 0.05)                                                                                                                                             \\
		\hline                                                                                                                                                                                                                
				
		NB + TF-IDF  & 0.79 \newline ($\pm$ 0.06) & 0.74 \newline ($\pm$ 0.07) & 0.87 \newline ($\pm$ 0.07) & 0.80 \newline ($\pm$ 0.04) & 0.79 \newline ($\pm$ 0.04)  & 0.62 \newline ($\pm$ 0.04)  & 0.80 \newline ($\pm$ 0.04)  \\
		\hline
		
		SVM + TF-IDF  & 0.80 \newline ($\pm$ 0.03) & 0.70 \newline ($\pm$ 0.02) & 0.84 \newline ($\pm$ 0.01) & 0.84 \newline ($\pm$ 0.05) & 0.80 \newline ($\pm$ 0.02)  & 0.68 \newline ($\pm$ 0.02)  & 0.82 \newline ($\pm$ 0.03)                                                                                                                                             \\
		\hline                                                                                                                                              
		
		NB + GloVe  & 0.82 \newline ($\pm$ 0.00) & 0.78 \newline ($\pm$ 0.00) & 0.89 \newline ($\pm$ 0.00) & 0.82 \newline ($\pm$ 0.00) & 0.78 \newline ($\pm$ 0.00)  & 0.68 \newline ($\pm$ 0.00)   & 0.83 \newline ($\pm$ 0.01)  \\
		\hline
		
		SVM + GloVe  & 0.60 \newline ($\pm$ 0.29) & 0.77 \newline ($\pm$ 0.00) & 0.63 \newline ($\pm$ 0.37) & 0.61 \newline ($\pm$ 0.31) & 0.59 \newline ($\pm$ 0.25)  & 0.67 \newline ($\pm$ 0.01) & 0.81 \newline ($\pm$ 0.01)                                                                                                                                              \\
		\hline                                                                
		                                                                                                                                                                  
	\end{tabular}
	\caption{Micro averaged f1-Scores of baseline models}
	\label{table:BaselineAccuracy}
\end{table}

\textit{The following are draft results for BoW/Tf-idf with stopwords removed}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{1.7cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|}
		\hline
		Model & Criteria 1                                                                           & Criteria 2    & Criteria 3 & Criteria 4 & Criteria 5 & Criteria 6 & Criteria 7                                                                                                                                                                                                                                                                                                                                                          \\
		\hline
		NB + BoW  & 0.85 \newline ($\pm$ 0.06) &  0.79 \newline ($\pm$ 0.08)  & 0.90 \newline ($\pm$ 0.03)  &  0.84 \newline ($\pm$ 0.12)  &  0.80 \newline ($\pm$ 0.07)   &  0.74 \newline ($\pm$ 0.09)   &  0.80 \newline ($\pm$ 0.07)    \\
		\hline
		
		SVM + BoW  &  0.78 \newline ($\pm$ 0.07)  &  0.80 \newline ($\pm$ 0.11)  & 0.90 \newline ($\pm$ 0.04)  &  0.87 \newline ($\pm$ 0.08)  & 0.78 \newline ($\pm$ 0.08)   &  0.72 \newline ($\pm$ 0.08)   &  0.81 \newline ($\pm$ 0.06)  \\
		\hline                                                                                                                                                                                                                
				
		NB + TF-IDF  &  0.79 \newline ($\pm$ 0.01)  & 0.80 \newline ($\pm$ 0.02)  &  0.89 \newline ($\pm$ 0.00)  &  0.81 \newline ($\pm$ 0.02)  & 0.77 \newline ($\pm$ 0.01)   & 0.69 \newline ($\pm$ 0.02)   &  0.81 \newline ($\pm$ 0.02)   \\
		\hline
		
		SVM + TF-IDF  &  0.86 \newline ($\pm$ 0.06)  & 0.85 \newline ($\pm$ 0.08)  & 0.89 \newline ($\pm$ 0.02)  &  0.86 \newline ($\pm$ 0.08)  &  0.80 \newline ($\pm$ 0.08)   & 0.77 \newline ($\pm$ 0.08)   & 0.82 \newline ($\pm$ 0.07)  \\
		\hline                                                                                                                                                                                                                                                                                                                             
	\end{tabular}
	\caption{Average micro f1-Scores of baseline models with stopwords removed}
	\label{table:BaselineAccuracy}
\end{table}

\textit{The following are draft results using tf-idf weighted w2v}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{1.7cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|}
		\hline
		Model & Criteria 1                                                                           & Criteria 2    & Criteria 3 & Criteria 4 & Criteria 5 & Criteria 6 & Criteria 7                                                                                                                                                                                                                                                                                                                                                          \\
		\hline                                                                                                                                              
		
		NB + GloVe  & 0.78 \newline ($\pm$ 0.01) & 0.79 \newline ($\pm$ 0.00) & 0.90 \newline ($\pm$ 0.01) & 0.81 \newline ($\pm$ 0.01) & 0.77 \newline ($\pm$ 0.01)  & 0.68 \newline ($\pm$ 0.00)   & 0.81 \newline ($\pm$ 0.00)  \\
		\hline
		
		SVM + GloVe  & 0.41 \newline ($\pm$ 0.27) & 0.40 \newline ($\pm$ 0.28) & 0.90 \newline ($\pm$ 0.01) & 0.60 \newline ($\pm$ 0.30) & 0.77 \newline ($\pm$ 0.01)  & 0.44 \newline ($\pm$ 0.17) & 0.60 \newline ($\pm$ 0.29)                                                                                                                                              \\
		\hline
		
	\end{tabular}
	\caption{w2v with tf-idf weights}
	\label{table:w2vtfidf}
\end{table}



\begin{figure}[H]
	\centering
	\includegraphics[totalheight=6cm]{images/classifier-performance-figure-draft.jpg}
	\caption{Effects of dataset size }
	\label{fig:AccruracyFig}
\end{figure}

\subsection{Deep Learning Model}
\label{sec:DLModelPerformanceResults}

epochs = 5 \newline
layers = 4 \newline
units per layer = 2500 \newline
batch size = 100 \newline
embedding size = 150 \newline

\begin{table}[H]
	\centering
	\begin{tabular}{|p{1.7cm}|p{1.65cm}|p{1.65cm}|p{1.6cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|}
		\hline
		Language Model & Criteria 1                                                                           & Criteria 2    & Criteria 3 & Criteria 4 & Criteria 5 & Criteria 6 & Criteria 7                                                                                                                                                                                                                                                                                                                                                          \\
		\hline       
		
		
		PT Wiki  & 0.38 & 0.36 & 0.41 & 0.37 & 0.36  & 0.38  & 0.39  \\
		\hline
		
		PT + FT Wiki  & 0.42 & 0.39 & 0.44 & 0.40 & 0.41 & 0.41  & 0.43   \\
		\hline
		
		PT Corpus  & 0.12 & 0.10 & 0.09 & 0.11 & 0.12  & 0.10 & 0.08\\
		\hline
		
		
		
	\end{tabular}
	\caption{\textit{Best performing accuracy scores}}
	\label{table:DLPerformance}
\end{table}

\section{Training Time}
\label{sec:TrainingTime}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{3cm}|p{5.5cm}|}
		\hline
		Language Model & Epoch Completion (minutes)                                                                                                                                                                                                                                                                                                                                                    \\
		\hline                                                                                                                                              
		
		PT Wiki  & 271  \\
		\hline
		
		PT + FT Wiki  & 292  \\
		\hline
		
		
		
	\end{tabular}
	\caption{\textit{Insert Caption}}
	\label{table:DLTrainingTime}
\end{table}

\section{Storage Requirements}
\label{sec:StorageRequirements}

\textit{This will probably be focusing the features used rather than the model itself}

The performance listed along with the varying parameters of the baseline features will be obtained from the respective best performing classifier as identified in \ref{sec:ClassificationPerformance}

\subsubsection{Bag Of Words}
\label{sec:BoWStorageRequirements}

\begin{table}[H]
	
	\begin{tabular}{|p{1.7cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|p{1.6cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|}
		\hline
		Vocabulary Size & Storage Size (MB) & Criteria 1                                                                           & Criteria 2    & Criteria 3 & Criteria 4 & Criteria 5 & Criteria 6 & Criteria 7                                                                                                                                                                                                                                                                                                                                                          \\
		\hline      
		
		5,000  & 100 & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY)  & 0.XX \newline ($\pm$ 0.YY)   & 0.XX \newline ($\pm$ 0.YY)  \\
		\hline
		
		10,000  & 100 & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY)  & 0.XX \newline ($\pm$ 0.YY)   & 0.XX \newline ($\pm$ 0.YY)  \\
		\hline 
		
		15,000  & 100 & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY)  & 0.XX \newline ($\pm$ 0.YY)   & 0.XX \newline ($\pm$ 0.YY)  \\
		\hline                                                                                                                                    
		
		20,000  & 100 & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY)  & 0.XX \newline ($\pm$ 0.YY)   & 0.XX \newline ($\pm$ 0.YY)  \\
		\hline
		
		
		
	\end{tabular}
	\caption{\textit{Insert caption}}
	\label{table:BoWStorage}
\end{table}

\subsubsection{TF-IDF}
\label{sec:TFIDFStorageRequirements}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{1.7cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|p{1.6cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|}
		\hline
		Vocabulary Size & Storage Size (MB) & Criteria 1                                                                           & Criteria 2    & Criteria 3 & Criteria 4 & Criteria 5 & Criteria 6 & Criteria 7                                                                                                                                                                                                                                                                                                                                                          \\
		\hline      
		
		5,000  & 100 & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY)  & 0.XX \newline ($\pm$ 0.YY)   & 0.XX \newline ($\pm$ 0.YY)  \\
		\hline
		
		10,000  & 100 & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY)  & 0.XX \newline ($\pm$ 0.YY)   & 0.XX \newline ($\pm$ 0.YY)  \\
		\hline 
		
		
		
	\end{tabular}
	\caption{\textit{Insert caption}}
	\label{table:TFIDFStorage}
\end{table}

\subsubsection{GloVe}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{1.7cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|p{1.6cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|p{1.65cm}|}
		\hline
		Vector Length & Storage Size (MB) & Criteria 1                                                                           & Criteria 2    & Criteria 3 & Criteria 4 & Criteria 5 & Criteria 6 & Criteria 7                                                                                                                                                                                                                                                                                                                                                          \\
		\hline                                                                                                                                               
		
		200  & 100 & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY) & 0.XX \newline ($\pm$ 0.YY)  & 0.XX \newline ($\pm$ 0.YY)   & 0.XX \newline ($\pm$ 0.YY)  \\
		\hline

		
		
	\end{tabular}
	\caption{\textit{Insert caption}}
	\label{table:w2vStorage}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{3cm}|p{2cm}|}
		\hline
		Language Model & Size (MB)                                                                                                                                                                                                                                                                                                                                                    \\
		\hline                                                                                                                                              
		
		PT Wiki  & 516  \\
		\hline
		
		PT + FT Wiki  & 599  \\
		\hline
		
		
		
	\end{tabular}
	\caption{\textit{Insert Caption}}
	\label{table:DLStorageRequirements}
\end{table}


\chapter{Discussion}
\label{chap:Discussion}

\section{Model Performance}
\label{sec:ModelPerformance}
\textit{Discuss and highlight the key points of what we have learnt based on the results obtained. E.g. The performance of the classifiers across the board, its performance relative to the other criteria (i.e. how the subjectivity of the criteria affected the performance of the classifier)}

\section{Real World Applications}
\label{sec:RealWorldApplication}
\textit{This is where I'll be talking about the performance of constructing and testing the low credibility classifiers}

\chapter{Conclusions and Future Work}
\label{chap:Conclusions}
This section will be completed after the fulfilment of the proposed work detailed in Section \ref{chap:ProposedWork}

\section{Future Work}
\label{FutureWork}

\section{Conclusions}
\label{sec:ConclusionsConclusions}



\clearemptydoublepage

%\phantomsection \addcontentsline{toc}{chapter}{Index}
% \renewcommand{\baselinestretch}{1} \small \normalsize
% \printindex

\appendix
\chapter{Exmaple}
\section{Overview}
This is an example entry in the appendix



%\input{Bibliography/biblio3}
\bibliographystyle{IEEEtranS}
%\bibliographystyle{acm}
\bibliography{my_reference}
%\bibliography{Bibliography/biblio4}


\end{document}
